{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.bhno import load_file, create_inputs\n",
    "th.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']\n",
    "orig_train_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/train/4.mat')\n",
    "train_cnt = orig_train_cnt.reorder_channels(sensor_names)\n",
    "\n",
    "train_inputs = create_inputs(train_cnt, final_hz=256, half_before=True)\n",
    "n_split = len(train_inputs[0]) - 40\n",
    "test_inputs = [t[-40:] for t in train_inputs]\n",
    "train_inputs = [t[:-40] for t in train_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "if cuda:\n",
    "    train_inputs = [i.cuda() for i in train_inputs]\n",
    "    test_inputs = [i.cuda() for i in test_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.graph import Node\n",
    "from reversible2.branching import CatChans, ChunkChans, Select\n",
    "def invert(feature_model, out):\n",
    "    return feature_model.invert(out)\n",
    "\n",
    "from copy import deepcopy\n",
    "from reversible2.graph import Node\n",
    "from reversible2.distribution import TwoClassDist\n",
    "from reversible2.wrap_invertible import WrapInvertible\n",
    "from reversible2.blocks import dense_add_const, conv_add_3x3_const\n",
    "from reversible2.rfft import RFFT, Interleave\n",
    "from reversible2.util import set_random_seeds\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "set_random_seeds(2019011641, cuda)\n",
    "n_chans = train_inputs[0].shape[1]\n",
    "n_time = train_inputs[0].shape[2]\n",
    "base_model = nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "                   grad_is_inverse=True, keep_input=True),# 2 x 256\n",
    "    conv_add_3x3_const(2*n_chans,32),\n",
    "    conv_add_3x3_const(2*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True), grad_is_inverse=True), # 4 x 128\n",
    "    conv_add_3x3_const(4*n_chans,32),\n",
    "    conv_add_3x3_const(4*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True), grad_is_inverse=True), # 8 x 64\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    conv_add_3x3_const(8*n_chans,32, final_block=True))\n",
    "base_model.cuda();\n",
    "\n",
    "branch_1_a =  nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=False), \n",
    "                   grad_is_inverse=True, keep_input=True), # 8 x 32\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True),# 16 x 16\n",
    "    conv_add_3x3_const(16*n_chans,32),\n",
    "    conv_add_3x3_const(16*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True), # 32 x 8\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    conv_add_3x3_const(32*n_chans,32, final_block=True),\n",
    ")\n",
    "branch_1_b = nn.Sequential(\n",
    "    *(list(deepcopy(branch_1_a).children()) + [\n",
    "    WrapInvertible(ViewAs((-1, 32*n_chans,n_time//64,1), (-1,(n_time // 2)*n_chans)),\n",
    "                   grad_is_inverse=True, keep_input=True),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32, final_block=True),\n",
    "]))\n",
    "branch_1_a.cuda();\n",
    "branch_1_b.cuda();\n",
    "\n",
    "branch_2_a = nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1], chunk_chans_first=False),\n",
    "                   keep_input=True, grad_is_inverse=True),# 32 x 4\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True),# 64 x 2\n",
    "    conv_add_3x3_const(64*n_chans,32),\n",
    "    conv_add_3x3_const(64*n_chans,32),\n",
    "    WrapInvertible(ViewAs((-1, (n_time // 4)*n_chans,1,1), (-1,(n_time // 4)*n_chans)),\n",
    "                   grad_is_inverse=True),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64, final_block=True),\n",
    ")\n",
    "\n",
    "\n",
    "branch_2_b = deepcopy(branch_2_a).cuda()\n",
    "branch_2_a.cuda();\n",
    "branch_2_b.cuda();\n",
    "\n",
    "final_model = nn.Sequential(\n",
    "    dense_add_const(n_time*n_chans,256,keep_input=True),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    WrapInvertible(RFFT(), final_block=True),\n",
    ")\n",
    "final_model.cuda();\n",
    "o = Node(None, base_model)\n",
    "o = Node(o, ChunkChans(2))\n",
    "o1a = Node(o, Select(0))\n",
    "o1b = Node(o, Select(1))\n",
    "o1a = Node(o1a, branch_1_a)\n",
    "o1b = Node(o1b, branch_1_b)\n",
    "o2 = Node(o1a, ChunkChans(2))\n",
    "o2a = Node(o2, Select(0))\n",
    "o2b = Node(o2, Select(1))\n",
    "o2a = Node(o2a, branch_2_a)\n",
    "o2b = Node(o2b, branch_2_b)\n",
    "o = Node([o1b,o2a,o2b], CatChans())\n",
    "o = Node(o, final_model)\n",
    "feature_model = o\n",
    "if cuda:\n",
    "    feature_model.cuda()\n",
    "feature_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.constantmemory import clear_ctx_dicts\n",
    "from reversible2.distribution import TwoClassDist\n",
    "\n",
    "feature_model.data_init(th.cat((train_inputs[0], train_inputs[1]), dim=0))\n",
    "\n",
    "# Check that forward + inverse is really identical\n",
    "t_out = feature_model(train_inputs[0][:2])\n",
    "inverted = invert(feature_model, t_out)\n",
    "clear_ctx_dicts(feature_model)\n",
    "assert th.allclose(train_inputs[0][:2], inverted, rtol=1e-3,atol=1e-4)\n",
    "device = list(feature_model.parameters())[0].device\n",
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "class_dist = TwoClassDist(2, np.prod(train_inputs[0].size()[1:]) - 2)\n",
    "class_dist.cuda()\n",
    "\n",
    "for i_class in range(2):\n",
    "    with th.no_grad():\n",
    "        this_outs = feature_model(train_inputs[i_class])\n",
    "        mean = th.mean(this_outs, dim=0)\n",
    "        std = th.std(this_outs, dim=0)\n",
    "        class_dist.set_mean_std(i_class, mean, std)\n",
    "        # Just check\n",
    "        setted_mean, setted_std = class_dist.get_mean_std(i_class)\n",
    "        assert th.allclose(mean, setted_mean)\n",
    "        assert th.allclose(std, setted_std)\n",
    "clear_ctx_dicts(feature_model)\n",
    "\n",
    "optim_model = th.optim.Adam(feature_model.parameters(), lr=1e-4, betas=(0,0.9))\n",
    "optim_dist = th.optim.Adam(class_dist.parameters(), lr=1e-2, betas=(0,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, F):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.F = F\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.F(x) + x\n",
    "\n",
    "\n",
    "from reversible2.discriminator import ProjectionDiscriminator\n",
    "def res_block(n_c, n_i_c):\n",
    "     return ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c, n_i_c, (3,1), stride=1, padding=(1,0),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c, (3,1), stride=1, padding=(1,0),bias=True)),\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "adv_model = nn.Sequential(\n",
    "    nn.Conv2d(22,64, (3,1), stride=1, padding=(1,0),bias=True), #256\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), #128\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 64\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 32\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 16\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 8\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 4\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 2\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    ViewAs((-1,64,2,1), (-1,128)),\n",
    "    )\n",
    "\n",
    "adv_model = ProjectionDiscriminator(adv_model,128,2,)\n",
    "adv_model.cuda();\n",
    "\n",
    "optim_adv = th.optim.Adam([{\n",
    "    'params': adv_model.parameters(),\n",
    "    'lr': 4e-4, 'weight_decay': 0.00}],#lr 0.0004\n",
    "                         betas=(0,0.9))\n",
    "from reversible2.graph import data_init_sequential_module\n",
    "_ = data_init_sequential_module(adv_model.adv_feature_model, th.cat((train_inputs[0], train_inputs[1]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "g_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "g_grad = np.nan\n",
    "g_grad_norm = np.nan\n",
    "d_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "gradient_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "d_grad = np.nan\n",
    "d_grad_norm = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_per_class = []\n",
    "with th.no_grad():\n",
    "    for i_class in range(2):\n",
    "        samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 4)\n",
    "        inverted = feature_model.invert(samples).detach()\n",
    "        inverted_per_class.append(inverted)\n",
    "clear_ctx_dicts(feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del score_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del score_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_real = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.constantmemory import clear_ctx_dicts\n",
    "from reversible2.timer import Timer\n",
    "from plot import plot_outs\n",
    "from reversible2.gradient_penalty import gradient_penalty\n",
    "\n",
    "\n",
    "i_start_epoch_out = 401\n",
    "n_epochs = 1001\n",
    "gen_frequency = 10\n",
    "for i_epoch in range(n_epochs):\n",
    "    epoch_row = {}\n",
    "    with Timer(name='EpochLoop', verbose=False) as loop_time:\n",
    "        gen_update = (i_epoch % gen_frequency) == (gen_frequency-1)\n",
    "            \n",
    "        optim_model.zero_grad()\n",
    "        optim_dist.zero_grad()\n",
    "        optim_adv.zero_grad()\n",
    "        for i_class in range(len(train_inputs)):\n",
    "            y = np_to_var([i_class]).cuda()\n",
    "            class_ins = train_inputs[i_class]\n",
    "            inverted = inverted_per_class[i_class]\n",
    "            score_fake = adv_model(inverted.detach(), y)\n",
    "            score_real = adv_model(class_ins, y)\n",
    "            gradient_loss = gradient_penalty(adv_model, class_ins, inverted[:(len(class_ins))].detach(), y)\n",
    "            d_loss =gradient_loss * 100 -score_real.mean() + score_fake.mean() \n",
    "            d_loss.backward()\n",
    "            # Clip gradient\n",
    "            wd_d = -(-score_real.mean() + score_fake.mean()).item()\n",
    "            epoch_row['wd_d_{:d}'.format(i_class)] = wd_d\n",
    "            d_grad_norm = np.mean([th.nn.utils.clip_grad_norm_([p],100) for p in adv_model.parameters()])\n",
    "            d_grad = np.mean([th.sum(p.grad **2).item() for p in adv_model.parameters()])\n",
    "            clear_ctx_dicts(feature_model)\n",
    "        optim_adv.step()\n",
    "    epoch_row.update({\n",
    "    'wd_d': wd_d,\n",
    "    'd_loss': d_loss.item(),\n",
    "    'g_loss': g_loss.item(),\n",
    "    'grad_loss': gradient_loss.item(),\n",
    "    'o_real': th.mean(score_real).item(),\n",
    "    'o_fake': th.mean(score_fake).item(),\n",
    "    'g_grad': g_grad,\n",
    "    'g_grad_norm': g_grad_norm,\n",
    "    'd_grad': d_grad,\n",
    "    'd_grad_norm': d_grad_norm,\n",
    "    'runtime': loop_time.elapsed_secs * 1000})\n",
    "    if i_epoch % (n_epochs // 20) != 0:\n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "        # otherwise add ot loss in\n",
    "    else:\n",
    "        for i_class in range(len(train_inputs)):\n",
    "            with th.no_grad():\n",
    "                inverted = inverted_per_class[i_class]\n",
    "                clear_ctx_dicts(feature_model)\n",
    "                ot_loss_in = ot_euclidean_loss_for_samples(class_ins.view(class_ins.shape[0], -1),\n",
    "                                                           inverted.view(inverted.shape[0], -1)[:(len(class_ins))])\n",
    "                epoch_row['ot_loss_in_{:d}'.format(i_class)] = ot_loss_in.item()\n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "        print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "        print(\"G Loss: {:.2E}\".format(g_loss.item()))\n",
    "        print(\"D Loss: {:.2E}\".format(d_loss.item()))\n",
    "        print(\"OT Loss In: {:.2E}\".format(ot_loss_in.item()))\n",
    "        display(df.iloc[-3:])\n",
    "        #print(\"OT Loss Out: {:.2E}\".format(ot_loss_out.item()))\n",
    "        #print(\"Transformed OT Loss In: {:.2E}\".format(ot_transformed_in.item()))\n",
    "        #print(\"Transformed OT Loss Out: {:.2E}\".format(ot_transformed_out.item()))\n",
    "        print(\"Loop Time: {:.0f} ms\".format(loop_time.elapsed_secs * 1000))\n",
    "        plot_outs(feature_model, train_inputs, test_inputs,\n",
    "                 class_dist)\n",
    "        fig = plt.figure(figsize=(8,2))\n",
    "        plt.plot(var_to_np(th.cat((th.exp(class_dist.class_log_stds),\n",
    "                                 th.exp(class_dist.non_class_log_stds)))),\n",
    "                marker='o')\n",
    "        display_close(fig)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    i_class = 0\n",
    "    s_a_o = train_inputs[0]\n",
    "    s_b_o = inverted_per_class[0]\n",
    "    s_a = s_a_o.view(s_a_o.shape[0], -1)\n",
    "    s_b = s_b_o[:len(s_a_o)].view(s_a_o.shape[0], -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    diffs = s_a.unsqueeze(1) - s_b.unsqueeze(0)\n",
    "    diffs = th.sqrt(th.clamp(th.sum(diffs * diffs, dim=2), min=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "transport_mat = ot.emd([], [], var_to_np(diffs))\n",
    "\n",
    "\n",
    "transport_mat = transport_mat * (transport_mat >= (0.5 / (diffs.numel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(transport_mat > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mat = np_to_var(transport_mat, dtype=np.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.sum(t_mat * diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.max(t_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_a = adv_model(s_a_o, np_to_var([i_class], dtype=np.int64, device='cuda'))\n",
    "score_b = adv_model(s_b_o, np_to_var([i_class], dtype=np.int64, device='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.sqrt(th.sum(s_a_o ** 2, dim=tuple(range(s_a_o.ndimension()))[1:]) + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.norm(s_a_o, p=2, dim=tuple(range(s_a_o.ndimension()))[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.norm_except_dim(s_a_o, p=2, dim=tuple(range(s_a_o.ndimension()))[1:]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check correctness\n",
    "all_dists = []\n",
    "for i_a, i_b in zip(*np.where(transport_mat)):\n",
    "    all_dists.append(diffs[i_a, i_b])\n",
    "\n",
    "th.mean(th.stack(all_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diffs = []\n",
    "for i_a, i_b in zip(*np.where(transport_mat)):\n",
    "    score_diffs.append(score_a[i_a] - score_b[i_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at one specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = s_a_o[0]\n",
    "b = s_b_o[np.where(transport_mat)[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.sqrt(th.sum((b-a) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_a = adv_model(a.unsqueeze(0), np_to_var([i_class], dtype=np.int64, device='cuda'))\n",
    "score_b = adv_model(b.unsqueeze(0), np_to_var([i_class], dtype=np.int64, device='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_a - score_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "def grad_norm(critic, points, y=None, max_grad=1):\n",
    "    if y is None:\n",
    "        score = critic(points)\n",
    "    else:\n",
    "        score = critic(points, y)\n",
    "\n",
    "    # Calculate gradients of scores with respect to examples\n",
    "    gradients = autograd.grad(\n",
    "        outputs=score,\n",
    "        inputs=points,\n",
    "        grad_outputs=th.ones(score.size(), device=score.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = th.sqrt(th.sum(gradients ** 2, dim=tuple(range(test_a_o.ndimension()))[1:]) + 1e-12)\n",
    "    return gradients_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_a_o = s_a_o.detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_norm = grad_norm(adv_model, test_a_o, np_to_var([i_class], dtype=np.int64, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.sqrt(th.sum(g_norm **2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
