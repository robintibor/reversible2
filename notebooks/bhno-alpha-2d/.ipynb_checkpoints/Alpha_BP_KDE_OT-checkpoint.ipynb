{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "from reversible2.invert import invert\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.bhno import create_inputs, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/train/4.mat')\n",
    "train_cnt = orig_train_cnt.reorder_channels(['C3', 'C4'])\n",
    "\n",
    "train_inputs = create_inputs(train_cnt, final_hz=64, half_before=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/test/4.mat')\n",
    "test_cnt = orig_test_cnt.reorder_channels(['C3', 'C4'])\n",
    "test_inputs = create_inputs(test_cnt, final_hz=64, half_before=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = False\n",
    "if cuda:\n",
    "    train_inputs = [i.cuda() for i in train_inputs]\n",
    "    test_inputs = [i.cuda() for i in test_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.plot(var_to_np(th.mean(th.mean(th.abs(th.rfft(train_inputs[0].squeeze(),signal_ndim=1)), dim=-1), dim=0)).T)\n",
    "plt.plot(var_to_np(th.mean(th.mean(th.abs(th.rfft(train_inputs[1].squeeze(),signal_ndim=1)), dim=-1), dim=0)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bps_train = [th.mean(th.mean(th.abs(th.rfft(ins.squeeze(),signal_ndim=1)), dim=-1)[:,:,9:14], dim=-1)\n",
    " for ins in train_inputs]\n",
    "\n",
    "bps_test = [th.mean(th.mean(th.abs(th.rfft(ins.squeeze(),signal_ndim=1)), dim=-1)[:,:,9:14], dim=-1)\n",
    " for ins in test_inputs]\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(var_to_np(bps_train[0])[:,0], var_to_np(bps_train[0])[:,1])\n",
    "plt.scatter(var_to_np(bps_train[1])[:,0], var_to_np(bps_train[1])[:,1])\n",
    "\n",
    "\n",
    "plt.scatter(var_to_np(bps_test[0])[:,0], var_to_np(bps_test[0])[:,1])\n",
    "plt.scatter(var_to_np(bps_test[1])[:,0], var_to_np(bps_test[1])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(var_to_np(bps_train[0])[:30,0], var_to_np(bps_train[0])[:30,1])\n",
    "plt.scatter(var_to_np(bps_train[1])[:30,0], var_to_np(bps_train[1])[:30,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.distribution import TwoClassDist\n",
    "\n",
    "from reversible2.blocks import dense_add_block, conv_add_block_3x3\n",
    "from reversible2.util import set_random_seeds\n",
    "import torch as th\n",
    "cuda = False\n",
    "set_random_seeds(2019011641, cuda)\n",
    "feature_model = nn.Sequential(\n",
    "    dense_add_block(2,64),\n",
    "    dense_add_block(2,64),\n",
    "    dense_add_block(2,64),\n",
    "    dense_add_block(2,64),\n",
    ")\n",
    "ref_log_stds = [th.zeros((len(ins),2), requires_grad=True) for ins in bps_train]\n",
    "optim_stds = th.optim.Adam(ref_log_stds, lr=1e-2)\n",
    "\n",
    "optim_model = th.optim.Adam(feature_model.parameters())\n",
    "class_dist = TwoClassDist(2,0)\n",
    "optim_dist = th.optim.Adam(class_dist.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import braindecode\n",
    "\n",
    "from reversible2.plot import display_close\n",
    "def get_prob(x, dists):\n",
    "    return th.mean(th.stack([th.exp(dist.log_prob(x)) for dist in dists]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "from reversible2.plot import display_text\n",
    "from matplotlib.patches import Ellipse\n",
    "# lets do first class first\n",
    "n_epochs = 501\n",
    "\n",
    "i_class = 0\n",
    "\n",
    "rng = RandomState(30984)\n",
    "\n",
    "\n",
    "class_ins = bps_train[i_class]\n",
    "for i_epoch in range(n_epochs):\n",
    "    inds_per_batch = get_balanced_batches(len(class_ins),rng=rng,shuffle=True, batch_size=100)\n",
    "    if i_epoch > -1:\n",
    "        for i_examples in inds_per_batch:\n",
    "            i_others = np.setdiff1d(np.arange(len(class_ins)), i_examples)\n",
    "\n",
    "            testing_examples = class_ins[i_examples]\n",
    "            reference_examples = class_ins[i_others]\n",
    "\n",
    "            reference_stds = th.exp(ref_log_stds[i_class][i_others])\n",
    "            reference_out = feature_model(reference_examples)\n",
    "            testing_out = feature_model(testing_examples)\n",
    "\n",
    "            dists = [th.distributions.MultivariateNormal(m, covariance_matrix=th.diag(s * s))\n",
    "                for m, s in zip(reference_out, reference_stds)]\n",
    "            likelihoods = get_prob(testing_out, dists)\n",
    "            #likelihood_loss = -th.mean(likelihoods)\n",
    "            likelihood_loss = -th.mean(th.log(likelihoods))\n",
    "            optim_stds.zero_grad()\n",
    "            likelihood_loss.backward()\n",
    "            optim_stds.step()\n",
    "    if i_epoch > -1:\n",
    "        all_outs = feature_model(class_ins).detach()\n",
    "        dists = [th.distributions.MultivariateNormal(m, covariance_matrix=th.diag(s * s))\n",
    "                        for m, s in zip(all_outs, th.exp(ref_log_stds[i_class]))]\n",
    "\n",
    "        real_samples = [d.sample([4]) for d in dists]\n",
    "        real_samples = th.cat(real_samples).detach()\n",
    "        inverted_real = invert(feature_model, real_samples).detach()\n",
    "\n",
    "        gen_samples = class_dist.get_samples(i_class, len(real_samples))\n",
    "        inverted_gen = invert(feature_model, gen_samples)\n",
    "\n",
    "        ot_loss = ot_euclidean_loss_for_samples(inverted_real, inverted_gen)\n",
    "        optim_dist.zero_grad()\n",
    "        optim_model.zero_grad()\n",
    "        ot_loss.backward()\n",
    "        optim_dist.step()\n",
    "        optim_model.step()\n",
    "        \n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        display_text(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "        display_text(\"Likelihood loss {:5.2E}\".format(likelihood_loss.item()))\n",
    "        display_text(\"OT loss               {:5.2E}\".format(ot_loss.item()))\n",
    "\n",
    "        all_outs = feature_model(class_ins)\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        gen_samples = class_dist.get_samples(i_class, len(real_samples))\n",
    "        plt.scatter(var_to_np(all_outs)[:,0], var_to_np(all_outs)[:,1],s=5)\n",
    "        plt.scatter(var_to_np(gen_samples)[:,0], var_to_np(gen_samples)[:,1],s=5)\n",
    "\n",
    "        x_start, x_stop = (th.floor(th.min(th.cat((all_outs[:,0], gen_samples[:,0])))).item(),\n",
    "            th.ceil(th.max(th.cat((all_outs[:,0], gen_samples[:,0])))).item())\n",
    "\n",
    "        y_start, y_stop = (th.floor(th.min(th.cat((all_outs[:,1], gen_samples[:,1])))).item(),\n",
    "            th.ceil(th.max(th.cat((all_outs[:,1], gen_samples[:,1])))).item())\n",
    "        xs = th.linspace(x_start, x_stop, 20)\n",
    "        ys = th.linspace(y_start, y_stop, 20)\n",
    "\n",
    "        grid_xs, grid_ys = th.meshgrid(xs, ys)\n",
    "        x_ys = th.stack((grid_xs.contiguous().view(-1), grid_ys.contiguous().view(-1)), dim=-1)\n",
    "        dists = [th.distributions.MultivariateNormal(m, covariance_matrix=th.diag(s * s))\n",
    "                for m, s in zip(all_outs, th.exp(ref_log_stds[i_class]))]\n",
    "        probs = get_prob(x_ys, dists)\n",
    "        #plt.scatter(var_to_np(x_ys)[:,0], var_to_np(x_ys)[:,1],c=var_to_np(probs), cmap=cm.Reds)\n",
    "        plt.imshow(var_to_np(probs.view(20,20)).T, origin='lower left',\n",
    "                   extent=((x_start, x_stop, y_start, y_stop)), cmap=cm.Reds,\n",
    "                  aspect='auto', interpolation='bilinear', vmin=0)\n",
    "        plt.legend(('Real Outs', \"Samples\"), bbox_to_anchor=(1,1,0,0))\n",
    "\n",
    "        ax = plt.gca()\n",
    "        for m,s in zip(var_to_np(all_outs), var_to_np(th.exp(ref_log_stds[i_class]))):\n",
    "            ellipse = Ellipse(m, s[0], s[1], lw=0.5, alpha=0.5)\n",
    "            ax.add_artist(ellipse)\n",
    "            ellipse.set_edgecolor(seaborn.color_palette()[i_class])\n",
    "            ellipse.set_facecolor(\"None\")\n",
    "        plt.title(\"Latent Space\")\n",
    "        display_close(fig)\n",
    "        \n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        plt.scatter(var_to_np(inverted_real)[:,0], var_to_np(inverted_real)[:,1],s=5)\n",
    "        plt.scatter(var_to_np(inverted_gen)[:,0], var_to_np(inverted_real)[:,1],s=5)\n",
    "        plt.scatter(var_to_np(class_ins)[:,0], var_to_np(class_ins)[:,1],s=5)\n",
    "        plt.legend(('\"Real\"', \"Fake\", \"Real\"), bbox_to_anchor=(1,1,0,0))\n",
    "        plt.title(\"Input Space\")\n",
    "        display_close(fig)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(var_to_np(all_outs)[:,0], var_to_np(all_outs)[:,1],s=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outs = feature_model(class_ins).detach()\n",
    "dists = [th.distributions.MultivariateNormal(m, covariance_matrix=th.diag(s * s))\n",
    "                for m, s in zip(all_outs, th.exp(ref_log_stds[i_class]))]\n",
    "\n",
    "real_samples = [d.sample([4]) for d in dists]\n",
    "real_samples = th.cat(real_samples).detach()\n",
    "inverted_real = invert(feature_model, real_samples)\n",
    "\n",
    "gen_samples = class_dist.get_samples(i_class, len(real_samples))\n",
    "inverted_gen = invert(feature_model, gen_samples)\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(var_to_np(inverted_real)[:,0], var_to_np(inverted_real)[:,1],s=5)\n",
    "plt.scatter(var_to_np(inverted_gen)[:,0], var_to_np(inverted_gen)[:,1],s=5)\n",
    "plt.scatter(var_to_np(class_ins)[:,0], var_to_np(class_ins)[:,1],s=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
