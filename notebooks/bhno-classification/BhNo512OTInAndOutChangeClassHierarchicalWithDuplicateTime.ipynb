{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.bhno import load_file, create_inputs\n",
    "th.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/train/4.mat')\n",
    "train_cnt = orig_train_cnt.reorder_channels(['C3',])\n",
    "\n",
    "train_inputs = create_inputs(train_cnt, final_hz=512, half_before=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/test/4.mat')\n",
    "test_cnt = orig_test_cnt.reorder_channels(['C3', ])\n",
    "test_inputs = create_inputs(test_cnt, final_hz=512, half_before=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.branching import CatChans, ChunkChans, Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "if cuda:\n",
    "    train_inputs = [i.cuda() for i in train_inputs]\n",
    "    test_inputs = [i.cuda() for i in test_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.graph import Node\n",
    "from reversible2.branching import CatChans, ChunkChans, Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invert(feature_model, samples):\n",
    "    return feature_model.invert(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = th.linspace(0,8,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PadChans(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(PadChans, self).__init__()\n",
    "        self.n_chans = n_chans\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pad = th.zeros((x.size()[0], self.n_chans, x.shape[2], x.shape[3]), device=x.device)\n",
    "        out = th.cat((x,pad), dim=1)\n",
    "        return out\n",
    "\n",
    "    def invert(self, y):\n",
    "        return y[:,:-self.n_chans]\n",
    "\n",
    "class RepeatTime(nn.Module):\n",
    "    def __init__(self, n_times):\n",
    "        super(RepeatTime, self).__init__()\n",
    "        self.n_times = n_times\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(3).repeat(1,1,1,self.n_times,1).view(\n",
    "            x.shape[0],x.shape[1],x.shape[2] * 2, x.shape[3])\n",
    "        return x\n",
    "    \n",
    "    def invert(self, y):\n",
    "        return th.nn.functional.avg_pool2d(y,kernel_size=(2,1), stride=(2,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from reversible2.graph import Node\n",
    "from reversible2.distribution import TwoClassDist\n",
    "\n",
    "from reversible2.blocks import dense_add_block, conv_add_block_3x3\n",
    "from reversible2.rfft import RFFT, Interleave\n",
    "from reversible2.util import set_random_seeds\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "set_random_seeds(2019011641, cuda)\n",
    "\n",
    "base_model = nn.Sequential(\n",
    "    RepeatTime(2 ),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),# 2 x 512\n",
    "    conv_add_block_3x3(2,32),\n",
    "    conv_add_block_3x3(2,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 4 x 256\n",
    "    conv_add_block_3x3(4,32),\n",
    "    conv_add_block_3x3(4,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 8 x 128\n",
    "    conv_add_block_3x3(8,32),\n",
    "    conv_add_block_3x3(8,32))\n",
    "base_model.cuda();\n",
    "\n",
    "branch_1_a =  nn.Sequential(\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False), # 8 x 64\n",
    "    conv_add_block_3x3(8,32),\n",
    "    conv_add_block_3x3(8,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 16 x 32\n",
    "    conv_add_block_3x3(16,32),\n",
    "    conv_add_block_3x3(16,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 32 x 16\n",
    "    conv_add_block_3x3(32,32),\n",
    "    conv_add_block_3x3(32,32),\n",
    ")\n",
    "branch_1_b = nn.Sequential(\n",
    "    *(list(deepcopy(branch_1_a).children()) + [\n",
    "    ViewAs((-1, 32,16,1), (-1,512)),\n",
    "    dense_add_block(512,32),\n",
    "    dense_add_block(512,32),\n",
    "    dense_add_block(512,32),\n",
    "    dense_add_block(512,32),\n",
    "]))\n",
    "branch_1_a.cuda();\n",
    "branch_1_b.cuda();\n",
    "\n",
    "branch_2_a = nn.Sequential(\n",
    "    SubsampleSplitter(stride=[2,1], chunk_chans_first=False), # 32 x 8\n",
    "    conv_add_block_3x3(32,32),\n",
    "    conv_add_block_3x3(32,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 64 x 4\n",
    "    conv_add_block_3x3(64,32),\n",
    "    conv_add_block_3x3(64,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 128 x 2\n",
    "    conv_add_block_3x3(128,32),\n",
    "    conv_add_block_3x3(128,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=True), # 256 x 1\n",
    "    ViewAs((-1, 256,1,1), (-1,256)),\n",
    "    dense_add_block(256,64),\n",
    "    dense_add_block(256,64),\n",
    "    dense_add_block(256,64),\n",
    "    dense_add_block(256,64),\n",
    ")\n",
    "\n",
    "\n",
    "branch_2_b = deepcopy(branch_2_a).cuda()\n",
    "branch_2_a.cuda();\n",
    "branch_2_b.cuda();\n",
    "\n",
    "final_model = nn.Sequential(\n",
    "    dense_add_block(1024,256),\n",
    "    dense_add_block(1024,256),\n",
    "    dense_add_block(1024,256),\n",
    "    dense_add_block(1024,256),\n",
    "    RFFT()\n",
    ")\n",
    "final_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = Node(None, base_model)\n",
    "o = Node(o, ChunkChans(2))\n",
    "o1a = Node(o, Select(0))\n",
    "o1b = Node(o, Select(1))\n",
    "o1a = Node(o1a, branch_1_a)\n",
    "o1b = Node(o1b, branch_1_b)\n",
    "o2 = Node(o1a, ChunkChans(2))\n",
    "o2a = Node(o2, Select(0))\n",
    "o2b = Node(o2, Select(1))\n",
    "o2a = Node(o2a, branch_2_a)\n",
    "o2b = Node(o2b, branch_2_b)\n",
    "o = Node([o1b,o2a,o2b], CatChans())\n",
    "o = Node(o, final_model)\n",
    "feature_model = o\n",
    "if cuda:\n",
    "    feature_model.cuda()\n",
    "feature_model.eval()\n",
    "# Check that forward + inverse is really identical\n",
    "t_out = feature_model(train_inputs[0][:2])\n",
    "inverted = invert(feature_model, t_out)\n",
    "assert th.allclose(train_inputs[0][:2], inverted, rtol=1e-3,atol=1e-4)\n",
    "device = list(feature_model.parameters())[0].device\n",
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "class_dist = TwoClassDist(2, 2*np.prod(train_inputs[0].size()[1:]) - 2)\n",
    "class_dist.cuda()\n",
    "optim_model = th.optim.Adam(feature_model.parameters())\n",
    "optim_dist = th.optim.Adam(class_dist.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile plot.py\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from reversible2.util import var_to_np\n",
    "from reversible2.plot import display_close\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn\n",
    "\n",
    "def plot_outs(feature_model, train_inputs, test_inputs, class_dist):\n",
    "    \n",
    "    # Compute dist for mean/std of encodings\n",
    "    data_cls_dists = []\n",
    "    for i_class in range(len(train_inputs)):\n",
    "        this_class_outs = feature_model(train_inputs[i_class])[:,:2]\n",
    "        data_cls_dists.append(\n",
    "            th.distributions.MultivariateNormal(th.mean(this_class_outs, dim=0),\n",
    "            covariance_matrix=th.diag(th.std(this_class_outs, dim=0) ** 2)))\n",
    "    for setname, set_inputs in ((\"Train\", train_inputs), (\"Test\", test_inputs)):\n",
    "\n",
    "        outs = [feature_model(ins) for ins in set_inputs]\n",
    "        c_outs = [o[:,:2] for o in outs]\n",
    "\n",
    "        c_outs_all = th.cat(c_outs)\n",
    "\n",
    "        cls_dists = []\n",
    "        for i_class in range(len(c_outs)):\n",
    "            mean, std = class_dist.get_mean_std(i_class)\n",
    "            cls_dists.append(\n",
    "                th.distributions.MultivariateNormal(mean[:2],covariance_matrix=th.diag(std[:2] ** 2)))\n",
    "\n",
    "        preds_per_class = [th.stack([cls_dists[i_cls].log_prob(c_out)\n",
    "                         for i_cls in range(len(cls_dists))],\n",
    "                        dim=-1) for c_out in c_outs]\n",
    "\n",
    "        pred_labels_per_class = [np.argmax(var_to_np(preds), axis=1)\n",
    "                       for preds in preds_per_class]\n",
    "\n",
    "        labels = np.concatenate([np.ones(len(set_inputs[i_cls])) * i_cls \n",
    "         for i_cls in range(len(train_inputs))])\n",
    "\n",
    "        acc = np.mean(labels == np.concatenate(pred_labels_per_class))\n",
    "\n",
    "        data_preds_per_class = [th.stack([data_cls_dists[i_cls].log_prob(c_out)\n",
    "                         for i_cls in range(len(cls_dists))],\n",
    "                        dim=-1) for c_out in c_outs]\n",
    "        data_pred_labels_per_class = [np.argmax(var_to_np(data_preds), axis=1)\n",
    "                            for data_preds in data_preds_per_class]\n",
    "        data_acc = np.mean(labels == np.concatenate(data_pred_labels_per_class))\n",
    "\n",
    "        print(\"{:s} Accuracy: {:.1f}%\".format(setname, acc * 100))\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = plt.gca()\n",
    "        for i_class in range(len(c_outs)):\n",
    "            #if i_class == 0:\n",
    "            #    continue\n",
    "            o = var_to_np(c_outs[i_class]).squeeze()\n",
    "            incorrect_pred_mask = pred_labels_per_class[i_class] != i_class\n",
    "            plt.scatter(o[:,0], o[:,1], s=20, alpha=0.75, label=[\"Right\", \"Rest\"][i_class])\n",
    "            assert len(incorrect_pred_mask) == len(o)\n",
    "            plt.scatter(o[incorrect_pred_mask,0], o[incorrect_pred_mask,1], marker='x', color='black',\n",
    "                       alpha=1, s=5)\n",
    "            means, stds = class_dist.get_mean_std(i_class)\n",
    "            means = var_to_np(means)[:2]\n",
    "            stds = var_to_np(stds)[:2]\n",
    "            for sigma in [0.5,1,2,3]:\n",
    "                ellipse = Ellipse(means, stds[0]*sigma, stds[1]*sigma)\n",
    "                ax.add_artist(ellipse)\n",
    "                ellipse.set_edgecolor(seaborn.color_palette()[i_class])\n",
    "                ellipse.set_facecolor(\"None\")\n",
    "        for i_class in range(len(c_outs)):\n",
    "            o = var_to_np(c_outs[i_class]).squeeze()\n",
    "            plt.scatter(np.mean(o[:,0]), np.mean(o[:,1]),\n",
    "                       color=seaborn.color_palette()[i_class+2], s=80, marker=\"^\",\n",
    "                       label=[\"Right Mean\", \"Rest Mean\"][i_class])\n",
    "\n",
    "        plt.title(\"{:6s} Accuracy:        {:.1f}%\\n\"\n",
    "                  \"From data mean/std: {:.1f}%\".format(setname, acc * 100, data_acc * 100))\n",
    "        plt.legend(bbox_to_anchor=(1,1,0,0))\n",
    "        display_close(fig)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from plot import plot_outs\n",
    "\n",
    "i_start_epoch_out = 200\n",
    "n_epochs = 1001\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(name='EpochLoop', verbose=False) as loop_time:\n",
    "        optim_model.zero_grad()\n",
    "        optim_dist.zero_grad()\n",
    "        for i_class in range(len(train_inputs)):\n",
    "            class_ins = train_inputs[i_class]\n",
    "            samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 5)\n",
    "            inverted = feature_model.invert(samples)\n",
    "            outs = feature_model(class_ins)\n",
    "            if i_epoch < i_start_epoch_out:\n",
    "                ot_loss_out = th.zeros(1, device=class_ins.device)\n",
    "            else:\n",
    "                ot_loss_out = ot_euclidean_loss_for_samples(outs[:,:2].squeeze(), samples[:,:2].squeeze())\n",
    "            ot_loss_in = ot_euclidean_loss_for_samples(class_ins.squeeze(), inverted.squeeze())\n",
    "\n",
    "            other_class_ins = train_inputs[1-i_class]\n",
    "            changed_to_other_class = class_dist.change_to_other_class(outs, i_class_from=i_class, i_class_to=1-i_class)\n",
    "            other_inverted = feature_model.invert(changed_to_other_class)\n",
    "            ot_transformed_in = ot_euclidean_loss_for_samples(other_class_ins.squeeze(), other_inverted.squeeze())\n",
    "            if i_epoch < i_start_epoch_out:\n",
    "                ot_transformed_out = th.zeros(1, device=class_ins.device)\n",
    "            else:\n",
    "                other_samples = class_dist.get_samples(1-i_class, len(train_inputs[i_class]) * 5)\n",
    "                ot_transformed_out = ot_euclidean_loss_for_samples(changed_to_other_class[:,:2].squeeze(),\n",
    "                                                                   other_samples[:,:2].squeeze(),)\n",
    "            loss = ot_loss_in + ot_loss_out + ot_transformed_in + ot_transformed_out\n",
    "            loss.backward()\n",
    "        optim_model.step()\n",
    "        optim_dist.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "        print(\"Loss: {:.2E}\".format(loss.item()))\n",
    "        print(\"OT Loss In: {:.2E}\".format(ot_loss_in.item()))\n",
    "        print(\"OT Loss Out: {:.2E}\".format(ot_loss_out.item()))\n",
    "        print(\"Transformed OT Loss In: {:.2E}\".format(ot_transformed_in.item()))\n",
    "        print(\"Transformed OT Loss Out: {:.2E}\".format(ot_transformed_out.item()))\n",
    "        print(\"Loop Time: {:.0f} ms\".format(loop_time.elapsed_secs * 1000))\n",
    "        plot_outs(feature_model, train_inputs, test_inputs,\n",
    "                 class_dist)\n",
    "        fig = plt.figure(figsize=(8,2))\n",
    "        plt.plot(var_to_np(th.cat((th.exp(class_dist.class_log_stds),\n",
    "                                 th.exp(class_dist.non_class_log_stds)))),\n",
    "                marker='o')\n",
    "        display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_samples_factor in [5,1]:\n",
    "    for setname, set_inputs in ((\"Train\", train_inputs), (\"Test\", test_inputs)):\n",
    "        total_loss = 0\n",
    "        total_spec_ot = 0\n",
    "        for i_class in range(2):\n",
    "            class_ins = set_inputs[i_class]\n",
    "            samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * n_samples_factor)\n",
    "            inverted = feature_model.invert(samples)\n",
    "            loss = ot_euclidean_loss_for_samples(class_ins.squeeze(), inverted.squeeze())\n",
    "            total_loss += loss\n",
    "            spec_ot = ot_euclidean_loss_for_samples(\n",
    "                th.rfft(class_ins.squeeze(), signal_ndim=1, normalized=True).view(class_ins.shape[0],-1),\n",
    "                th.rfft(inverted.squeeze(), signal_ndim=1, normalized=True).view(inverted.shape[0],-1))\n",
    "            total_spec_ot += spec_ot\n",
    "        print(\"{:d} Samples \". format(len(samples)) + setname + \" Loss: {:.1f}\".format(total_loss.item() / 2))\n",
    "        print(\"{:d} Samples \". format(len(samples)) + setname + \" SpecLoss: {:.1f}\".format(spec_ot.item() / 2))\n",
    "\n",
    "# Show losses between sets\n",
    "for setname, set_inputs in ((\"Train\", train_inputs),):\n",
    "    total_loss = 0\n",
    "    total_spec_ot = 0\n",
    "    for i_class in range(2):\n",
    "        class_ins = set_inputs[i_class]\n",
    "        samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 5) # take same number of samples\n",
    "        inverted = test_inputs[i_class]\n",
    "        loss = ot_euclidean_loss_for_samples(class_ins.squeeze(), inverted.squeeze())\n",
    "        total_loss += loss\n",
    "        spec_ot = ot_euclidean_loss_for_samples(\n",
    "            th.rfft(class_ins.squeeze(), signal_ndim=1, normalized=True).view(class_ins.shape[0],-1),\n",
    "            th.rfft(inverted.squeeze(), signal_ndim=1, normalized=True).view(inverted.shape[0],-1))\n",
    "        total_spec_ot += spec_ot\n",
    "    print(setname + \" Loss: {:.1f}\".format(total_loss.item() / 2))\n",
    "    print(setname + \" SpecLoss: {:.1f}\".format(spec_ot.item() / 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(len(train_inputs)):\n",
    "    fig, axes = plt.subplots(5,4, figsize=(16,12), sharex=True, sharey=True)\n",
    "\n",
    "    for ax, curve in zip(axes.flatten(), var_to_np(train_inputs[i_class][:len(axes.flatten())]).squeeze()):\n",
    "        ax.plot(curve, color=seaborn.color_palette()[i_class])\n",
    "    display_close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(len(train_inputs)):\n",
    "    samples = class_dist.get_samples(i_class, 20)\n",
    "    inverted = feature_model.invert(samples)\n",
    "    fig, axes = plt.subplots(5,4, figsize=(16,12), sharex=True, sharey=True)\n",
    "\n",
    "    for ax, curve in zip(axes.flatten(), var_to_np(inverted).squeeze()):\n",
    "        ax.plot(curve, color=seaborn.color_palette()[i_class])\n",
    "    display_close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(len(train_inputs)):\n",
    "    samples = class_dist.get_samples(i_class, 2000)\n",
    "    inverted = feature_model.invert(samples)\n",
    "    amps_inv = th.sum(th.abs(th.rfft(inverted.squeeze(), 1, )), dim=-1)\n",
    "    amps_real =  th.sum(th.abs(th.rfft(train_inputs[i_class].squeeze(), 1, )), dim=-1)\n",
    "    fig = plt.figure(figsize=(8,3))\n",
    "    plt.plot(np.fft.rfftfreq(512, d=1/512.0), var_to_np(th.mean(amps_real, dim=0)))\n",
    "    plt.plot(np.fft.rfftfreq(512, d=1/512.0), var_to_np(th.mean(amps_inv, dim=0)))\n",
    "    plt.legend((\"Real\", \"Fake\"))\n",
    "    display_close(fig)\n",
    "    fig = plt.figure(figsize=(8,3))\n",
    "    plt.plot(np.fft.rfftfreq(512, d=1/512.0),\n",
    "             np.log(var_to_np(th.mean(amps_inv, dim=0))/ var_to_np(th.mean(amps_real, dim=0))))\n",
    "    plt.legend((\"Real\", \"Fake\"))\n",
    "    display_close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_th_grid(dim_0_vals, dim_1_vals):\n",
    "    curves = []\n",
    "    for dim_0_val in dim_0_vals:\n",
    "        this_curves = []\n",
    "        for dim_1_val in dim_1_vals:\n",
    "            vals = th.stack((dim_0_val, dim_1_val))\n",
    "            this_curves.append(vals)\n",
    "        curves.append(th.stack(this_curves))\n",
    "    curves = th.stack(curves)\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, i_sorted = th.sort(class_dist.get_mean_std(0,)[1][2:] + class_dist.get_mean_std(1,)[1][2:], descending=True)\n",
    "\n",
    "print(var_to_np(vals[:8]))\n",
    "print(var_to_np(i_sorted[:8]) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_dim_1 = 0\n",
    "i_dim_2 = 1\n",
    "means = th.stack([class_dist.get_mean_std(i_cls,)[0][[i_dim_1, i_dim_2]]\n",
    "     for i_cls in range(len(train_inputs))])\n",
    "stds = th.stack([class_dist.get_mean_std(i_cls,)[1][[i_dim_1, i_dim_2]]\n",
    "     for i_cls in range(len(train_inputs))])\n",
    "mins = th.min(means - stds * 2, dim=0)[0]\n",
    "maxs = th.max(means + stds * 2, dim=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "outs = [feature_model(ins) for ins in train_inputs]\n",
    "#c_outs = #[o[:,:] for o in outs]\n",
    "plt.figure(figsize=(5,5))\n",
    "for i_cls, c_out in enumerate(outs):\n",
    "    plt.scatter(var_to_np(c_out[:,i_dim_1]),\n",
    "                   var_to_np(c_out[:,i_dim_2]), s=50, alpha=0.75, label='Encodings {:s}'.format(\n",
    "\n",
    "                       [\"Right\", \"Rest\"][i_cls]))\n",
    "ax = plt.gca()\n",
    "rect = Rectangle(var_to_np(mins), maxs[0].item() - mins[0].item(),  maxs[1].item() - mins[1].item())\n",
    "ax.add_artist(rect)\n",
    "rect.set_edgecolor('black')\n",
    "rect.set_facecolor(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vals = 30\n",
    "dim_0_vals = th.linspace(mins[0].item(), maxs[0].item(),n_vals, device=mins.device)\n",
    "dim_1_vals = th.linspace(mins[1].item(), maxs[1].item(),n_vals, device=mins.device)\n",
    "\n",
    "\n",
    "grid = create_th_grid(dim_0_vals, dim_1_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_grid = class_dist.non_class_means.repeat(grid.shape[0],grid.shape[1],1)\n",
    "\n",
    "full_grid.data[:,:,i_dim_1] = grid[:,:,0]\n",
    "full_grid.data[:,:,i_dim_2] = grid[:,:,1]\n",
    "inverted = invert(feature_model, full_grid.view(-1, full_grid.shape[-1])).squeeze()\n",
    "\n",
    "inverted_grid = inverted.view(len(dim_0_vals), len(dim_1_vals),-1)\n",
    "\n",
    "x_len, y_len = var_to_np(maxs - mins) / full_grid.shape[:-1]\n",
    "\n",
    "max_abs = th.max(th.abs(inverted_grid))\n",
    "\n",
    "y_factor =  (y_len / (2*max_abs)).item() * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,32))\n",
    "\n",
    "for i_x in range(full_grid.shape[0]):\n",
    "    for i_y in range(full_grid.shape[1]):\n",
    "        x_start = mins[0].item() + x_len * i_x + 0.1 * x_len\n",
    "        x_end = mins[0].item() + x_len * i_x + 0.9 * x_len\n",
    "        y_center = mins[1].item() + y_len * i_y + 0.5 * y_len\n",
    "        \n",
    "        curve = var_to_np(inverted_grid[i_x][i_y])\n",
    "        label = ''\n",
    "        if i_x == 0 and i_y == 0:\n",
    "            label = 'Generated data'\n",
    "        plt.plot(np.linspace(x_start, x_end, len(curve)),\n",
    "                 curve * y_factor + y_center, color='black',\n",
    "                label=label)\n",
    "outs = [feature_model(ins) for ins in train_inputs]\n",
    "#c_outs = [o[:,:2] for o in outs]\n",
    "\n",
    "plt.gca().set_autoscale_on(False)\n",
    "for i_cls, c_out in enumerate(outs):\n",
    "    plt.scatter(var_to_np(c_out[:,i_dim_1]),\n",
    "               var_to_np(c_out[:,i_dim_2]), s=200, alpha=0.75, label='Encodings {:s}'.format(\n",
    "               [\"Right\", \"Rest\"][i_cls]))\n",
    "    \n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,2))\n",
    "for i_class in range(2):\n",
    "    cur_mean, cur_std = class_dist.get_mean_std(i_class, )\n",
    "    inverted = invert(feature_model, cur_mean.unsqueeze(0))\n",
    "    plt.plot(var_to_np(inverted.squeeze()))\n",
    "plt.legend((\"Right Hand\", \"Rest\"))\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,24))\n",
    "\n",
    "outs = [feature_model(ins) for ins in train_inputs]\n",
    "c_outs = [o[:,:2] for o in outs]\n",
    "\n",
    "for i_cls, c_out in enumerate(c_outs):\n",
    "    for a_c_in, a_c_out in zip(train_inputs[i_cls], c_out):\n",
    "        curve = var_to_np(a_c_in.squeeze())\n",
    "        x_start = a_c_out[0].item() - x_len * 0.45\n",
    "        x_end = a_c_out[0].item() + x_len * 0.45\n",
    "        y_center = a_c_out[1].item()\n",
    "        plt.plot(np.linspace(x_start, x_end, len(curve)),\n",
    "                     curve * y_factor + y_center, color=seaborn.color_palette()[i_cls])\n",
    "plt.xlim(mins[0].item(), maxs[0].item())\n",
    "plt.ylim(mins[1].item(), maxs[1].item())\n",
    "plt.gca().set_autoscale_on(False)\n",
    "for i_cls, c_out in enumerate(c_outs):\n",
    "    plt.scatter(var_to_np(c_out[:,0]),\n",
    "               var_to_np(c_out[:,1]), s=200, alpha=0.75, label='Encodings {:s}'.format(\n",
    "               [\"Right\", \"Rest\"][i_cls]))\n",
    "    \n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,24))\n",
    "\n",
    "outs = [feature_model(ins) for ins in test_inputs]\n",
    "c_outs = [o[:,:2] for o in outs]\n",
    "\n",
    "for i_cls, c_out in enumerate(c_outs):\n",
    "    for a_c_in, a_c_out in zip(train_inputs[i_cls], c_out):\n",
    "        curve = var_to_np(a_c_in.squeeze())\n",
    "        x_start = a_c_out[0].item() - x_len * 0.45\n",
    "        x_end = a_c_out[0].item() + x_len * 0.45\n",
    "        y_center = a_c_out[1].item()\n",
    "        plt.plot(np.linspace(x_start, x_end, len(curve)),\n",
    "                     curve * y_factor + y_center, color=seaborn.color_palette()[i_cls])\n",
    "plt.xlim(mins[0].item(), maxs[0].item())\n",
    "plt.ylim(mins[1].item(), maxs[1].item())\n",
    "plt.gca().set_autoscale_on(False)\n",
    "for i_cls, c_out in enumerate(c_outs):\n",
    "    plt.scatter(var_to_np(c_out[:,0]),\n",
    "               var_to_np(c_out[:,1]), s=200, alpha=0.75, label='Encodings {:s}'.format(\n",
    "               [\"Right\", \"Rest\"][i_cls]))\n",
    "    \n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(len(train_inputs)):\n",
    "    class_ins = train_inputs[i_class]\n",
    "    outs = feature_model(class_ins)\n",
    "    cor_probs = th.exp(class_dist.get_class_log_prob(i_class, outs))\n",
    "    incor_probs = th.exp(class_dist.get_class_log_prob(1-i_class, outs))\n",
    "    changed_outs = class_dist.change_to_other_class(outs, i_class_from=i_class, i_class_to=1-i_class)\n",
    "    changed_inverted = invert(feature_model, changed_outs)\n",
    "    fig, axes = plt.subplots(6,4, figsize=(16,16), sharex=True, sharey=True)\n",
    "    cor_changed_probs = th.exp(class_dist.get_class_log_prob(i_class, changed_outs))\n",
    "    incor_changed_probs = th.exp(class_dist.get_class_log_prob(1-i_class, changed_outs))\n",
    "\n",
    "    for ax, original, changed, p1, p2,p3,p4 in zip(\n",
    "            axes.flatten(), var_to_np(class_ins).squeeze(), var_to_np(changed_inverted).squeeze(),\n",
    "            var_to_np(cor_probs), var_to_np(incor_probs),\n",
    "            var_to_np(cor_changed_probs), var_to_np(incor_changed_probs),\n",
    "    ):\n",
    "        ax.plot(original, color=seaborn.color_palette()[i_class])\n",
    "        ax.plot(changed, color=seaborn.color_palette()[1-i_class], alpha=0.75)\n",
    "        ax.set_title(\"{:.1f} vs. {:.1f}\\n{:.1f} vs. {:.1f}\".format(p1*100,p2*100,p3*100,p4*100))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
