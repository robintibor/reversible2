{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.bhno import load_file, create_inputs\n",
    "th.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']\n",
    "orig_train_cnt = load_file('/data/schirrmr/schirrmr/HGD-public/reduced/train/4.mat')\n",
    "train_cnt = orig_train_cnt.reorder_channels(sensor_names)\n",
    "\n",
    "train_inputs = create_inputs(train_cnt, final_hz=256, half_before=True)\n",
    "n_split = len(train_inputs[0]) - 40\n",
    "test_inputs = [t[-40:] for t in train_inputs]\n",
    "train_inputs = [t[:-40] for t in train_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "if cuda:\n",
    "    train_inputs = [i.cuda() for i in train_inputs]\n",
    "    test_inputs = [i.cuda() for i in test_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.graph import Node\n",
    "from reversible2.branching import CatChans, ChunkChans, Select\n",
    "def invert(feature_model, out):\n",
    "    return feature_model.invert(out)\n",
    "\n",
    "from copy import deepcopy\n",
    "from reversible2.graph import Node\n",
    "from reversible2.distribution import TwoClassDist\n",
    "from reversible2.wrap_invertible import WrapInvertible\n",
    "from reversible2.blocks import dense_add_const, conv_add_3x3_const\n",
    "from reversible2.rfft import RFFT, Interleave\n",
    "from reversible2.util import set_random_seeds\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "set_random_seeds(2019011641, cuda)\n",
    "n_chans = train_inputs[0].shape[1]\n",
    "n_time = train_inputs[0].shape[2]\n",
    "base_model = nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "                   grad_is_inverse=True, keep_input=True),# 2 x 256\n",
    "    conv_add_3x3_const(2*n_chans,32),\n",
    "    conv_add_3x3_const(2*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True), grad_is_inverse=True), # 4 x 128\n",
    "    conv_add_3x3_const(4*n_chans,32),\n",
    "    conv_add_3x3_const(4*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True), grad_is_inverse=True), # 8 x 64\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    conv_add_3x3_const(8*n_chans,32, keep_output=True))\n",
    "base_model.cuda();\n",
    "\n",
    "branch_1_a =  nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=False), \n",
    "                   grad_is_inverse=True, keep_input=True), # 8 x 32\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    conv_add_3x3_const(8*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True),# 16 x 16\n",
    "    conv_add_3x3_const(16*n_chans,32),\n",
    "    conv_add_3x3_const(16*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True), # 32 x 8\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    conv_add_3x3_const(32*n_chans,32, keep_output=True),\n",
    ")\n",
    "branch_1_b = nn.Sequential(\n",
    "    *(list(deepcopy(branch_1_a).children()) + [\n",
    "    WrapInvertible(ViewAs((-1, 32*n_chans,n_time//64,1), (-1,(n_time // 2)*n_chans)),\n",
    "                   grad_is_inverse=True, keep_input=True),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32),\n",
    "    dense_add_const((n_time // 2)*n_chans,32, keep_output=True),\n",
    "]))\n",
    "branch_1_a.cuda();\n",
    "branch_1_b.cuda();\n",
    "\n",
    "branch_2_a = nn.Sequential(\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1], chunk_chans_first=False),\n",
    "                   keep_input=True, grad_is_inverse=True),# 32 x 4\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    conv_add_3x3_const(32*n_chans,32),\n",
    "    WrapInvertible(SubsampleSplitter(stride=[2,1],chunk_chans_first=True),\n",
    "                   grad_is_inverse=True),# 64 x 2\n",
    "    conv_add_3x3_const(64*n_chans,32),\n",
    "    conv_add_3x3_const(64*n_chans,32),\n",
    "    WrapInvertible(ViewAs((-1, (n_time // 4)*n_chans,1,1), (-1,(n_time // 4)*n_chans)),\n",
    "                   grad_is_inverse=True),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64),\n",
    "    dense_add_const((n_time // 4)*n_chans,64, keep_output=True),\n",
    ")\n",
    "\n",
    "\n",
    "branch_2_b = deepcopy(branch_2_a).cuda()\n",
    "branch_2_a.cuda();\n",
    "branch_2_b.cuda();\n",
    "\n",
    "final_model = nn.Sequential(\n",
    "    dense_add_const(n_time*n_chans,256,keep_input=True),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    dense_add_const(n_time*n_chans,256),\n",
    "    WrapInvertible(RFFT(), keep_output=True),\n",
    ")\n",
    "final_model.cuda();\n",
    "o = Node(None, base_model)\n",
    "o = Node(o, ChunkChans(2))\n",
    "o1a = Node(o, Select(0))\n",
    "o1b = Node(o, Select(1))\n",
    "o1a = Node(o1a, branch_1_a)\n",
    "o1b = Node(o1b, branch_1_b)\n",
    "o2 = Node(o1a, ChunkChans(2))\n",
    "o2a = Node(o2, Select(0))\n",
    "o2b = Node(o2, Select(1))\n",
    "o2a = Node(o2a, branch_2_a)\n",
    "o2b = Node(o2b, branch_2_b)\n",
    "o = Node([o1b,o2a,o2b], CatChans())\n",
    "o = Node(o, final_model)\n",
    "feature_model = o\n",
    "if cuda:\n",
    "    feature_model.cuda()\n",
    "feature_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set feature model weights and distribution to good start parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.constantmemory import clear_ctx_dicts\n",
    "from reversible2.distribution import TwoClassDist\n",
    "\n",
    "feature_model.data_init(th.cat((train_inputs[0], train_inputs[1]), dim=0))\n",
    "\n",
    "# Check that forward + inverse is really identical\n",
    "t_out = feature_model(train_inputs[0][:2])\n",
    "inverted = invert(feature_model, t_out)\n",
    "clear_ctx_dicts(feature_model)\n",
    "assert th.allclose(train_inputs[0][:2], inverted, rtol=1e-3,atol=1e-4)\n",
    "device = list(feature_model.parameters())[0].device\n",
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "class_dist = TwoClassDist(2, np.prod(train_inputs[0].size()[1:]) - 2)\n",
    "class_dist.cuda()\n",
    "\n",
    "for i_class in range(2):\n",
    "    with th.no_grad():\n",
    "        this_outs = feature_model(train_inputs[i_class])\n",
    "        mean = th.mean(this_outs, dim=0)\n",
    "        std = th.std(this_outs, dim=0)\n",
    "        class_dist.set_mean_std(i_class, mean, std)\n",
    "        # Just check\n",
    "        setted_mean, setted_std = class_dist.get_mean_std(i_class)\n",
    "        assert th.allclose(mean, setted_mean)\n",
    "        assert th.allclose(std, setted_std)\n",
    "clear_ctx_dicts(feature_model)\n",
    "\n",
    "optim_model = th.optim.Adam(feature_model.parameters(), lr=1e-4, betas=(0,0.9))\n",
    "optim_dist = th.optim.Adam(class_dist.parameters(), lr=1e-2, betas=(0,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, F):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.F = F\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.F(x) + x\n",
    "\n",
    "\n",
    "from reversible2.discriminator import ProjectionDiscriminator\n",
    "def res_block(n_c, n_i_c):\n",
    "     return ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c, n_i_c, (3,1), stride=1, padding=(1,0),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c, (3,1), stride=1, padding=(1,0),bias=True)),\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "adv_model = nn.Sequential(\n",
    "    nn.Conv2d(22,64, (3,1), stride=1, padding=(1,0),bias=True), #256\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), #128\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 64\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 32\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 16\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 8\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 4\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    nn.AvgPool2d((2,1)), # 2\n",
    "    res_block(64,96),\n",
    "    res_block(64,96),\n",
    "    ViewAs((-1,64,2,1), (-1,128)),\n",
    "    )\n",
    "\n",
    "adv_model = ProjectionDiscriminator(adv_model,128,2,)\n",
    "adv_model.cuda();\n",
    "\n",
    "optim_adv = th.optim.Adam([{\n",
    "    'params': adv_model.parameters(),\n",
    "    'lr': 4e-4, 'weight_decay': 0.00}],#lr 0.0004\n",
    "                         betas=(0,0.9))\n",
    "from reversible2.graph import data_init_sequential_module\n",
    "_ = data_init_sequential_module(adv_model.adv_feature_model, th.cat((train_inputs[0], train_inputs[1]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "g_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "g_grad = np.nan\n",
    "g_grad_norm = np.nan\n",
    "d_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "gradient_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "d_grad = np.nan\n",
    "d_grad_norm = np.nan\n",
    "ot_out_loss_other = np_to_var([np.nan],dtype=np.float32)\n",
    "ot_out_loss = np_to_var([np.nan],dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.constantmemory import clear_ctx_dicts\n",
    "from reversible2.timer import Timer\n",
    "from plot import plot_outs\n",
    "from reversible2.gradient_penalty import gradient_penalty\n",
    "\n",
    "\n",
    "i_start_epoch_out = 401\n",
    "n_epochs = 5001\n",
    "gen_frequency = 10\n",
    "for i_epoch in range(n_epochs):\n",
    "    epoch_row = {}\n",
    "    with Timer(name='EpochLoop', verbose=False) as loop_time:\n",
    "        gen_update = (i_epoch % gen_frequency) == (gen_frequency-1)\n",
    "            \n",
    "        optim_model.zero_grad()\n",
    "        optim_dist.zero_grad()\n",
    "        optim_adv.zero_grad()\n",
    "        for i_class in range(len(train_inputs)):\n",
    "            y = np_to_var([i_class]).cuda()\n",
    "            class_ins = train_inputs[i_class]\n",
    "            other_class_ins = train_inputs[1-i_class]\n",
    "            with th.set_grad_enabled(gen_update):\n",
    "                samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 4)\n",
    "                inverted = feature_model.invert(samples)\n",
    "            \n",
    "            with th.set_grad_enabled(gen_update):\n",
    "                outs = feature_model(class_ins)\n",
    "                changed_to_other_class = class_dist.change_to_other_class(outs, i_class_from=i_class,\n",
    "                                                                          i_class_to=1-i_class)\n",
    "                other_inverted = feature_model.invert(changed_to_other_class)\n",
    "            if not gen_update:\n",
    "                score_fake = adv_model(inverted.detach(), y)\n",
    "                score_real = adv_model(class_ins, y)\n",
    "                gradient_loss = gradient_penalty(adv_model, class_ins, inverted[:(len(class_ins))].detach(), y)\n",
    "                d_loss = -score_real.mean() + score_fake.mean() + gradient_loss * 100\n",
    "                d_loss.backward()\n",
    "                \n",
    "                score_real_other = adv_model(other_class_ins.detach(), 1-y)\n",
    "                score_fake_other = adv_model(other_inverted.detach(), 1-y)\n",
    "                gradient_loss_other = gradient_penalty(adv_model, other_class_ins, \n",
    "                                                 other_inverted[:(len(class_ins))].detach(), 1-y)\n",
    "                d_loss_other =  -score_real_other.mean() + score_fake_other.mean() + gradient_loss_other * 100\n",
    "                d_loss_other.backward()\n",
    "                \n",
    "                # Clip gradient\n",
    "                d_grad_norm = np.mean([th.nn.utils.clip_grad_norm_([p],100) for p in adv_model.parameters()])\n",
    "                d_grad = np.mean([th.sum(p.grad **2).item() for p in adv_model.parameters()])\n",
    "                wd_d = -(-score_real.mean() + score_fake.mean()).item()\n",
    "                epoch_row['wd_d_{:d}'.format(i_class)] = wd_d\n",
    "            else:\n",
    "                ot_out_loss = ot_euclidean_loss_for_samples(samples[:,:2], outs[:,:2])\n",
    "                other_samples = class_dist.get_samples(1-i_class, len(train_inputs[1-i_class]) * 4)\n",
    "                ot_out_loss_other = ot_euclidean_loss_for_samples(other_samples[:,:2], changed_to_other_class[:,:2])\n",
    "                score_fake = adv_model(inverted, y)\n",
    "                score_fake_other = adv_model(other_inverted, 1-y)\n",
    "                g_loss = -th.mean(score_fake) - th.mean(score_fake_other) + ot_out_loss + ot_out_loss_other\n",
    "                g_loss.backward()\n",
    "                # Clip gradient\n",
    "                g_grad_norm = np.mean([th.nn.utils.clip_grad_norm_([p],100) for p in feature_model.parameters()])\n",
    "                g_grad = np.mean([th.sum(p.grad **2).item() for p in feature_model.parameters()])\n",
    "            clear_ctx_dicts(feature_model)\n",
    "                \n",
    "            \"\"\"\n",
    "            assert False\n",
    "            ot_loss_in = ot_euclidean_loss_for_samples(class_ins.view(class_ins.shape[0], -1),\n",
    "                                                       inverted.view(inverted.shape[0], -1))\n",
    "            del inverted\n",
    "            with th.set_grad_enabled(gen_update):\n",
    "                outs = feature_model(class_ins)\n",
    "            if i_epoch < i_start_epoch_out:\n",
    "                ot_loss_out = th.zeros(1, device=class_ins.device)\n",
    "            else:\n",
    "                ot_loss_out = ot_euclidean_loss_for_samples(outs[:,:2].squeeze(), samples[:,:2].squeeze())\n",
    "            del samples\n",
    "                \n",
    "            other_class_ins = train_inputs[1-i_class]\n",
    "            changed_to_other_class = class_dist.change_to_other_class(outs, i_class_from=i_class, i_class_to=1-i_class)\n",
    "            other_inverted = feature_model.invert(changed_to_other_class)\n",
    "            ot_transformed_in = ot_euclidean_loss_for_samples(other_class_ins.view(other_class_ins.shape[0], -1),\n",
    "                                                              other_inverted.view(other_inverted.shape[0], -1))\n",
    "            \n",
    "            if i_epoch < i_start_epoch_out:\n",
    "                ot_transformed_out = th.zeros(1, device=class_ins.device)\n",
    "            else:\n",
    "                other_samples = class_dist.get_samples(1-i_class, len(train_inputs[i_class]) * 2)\n",
    "                ot_transformed_out = ot_euclidean_loss_for_samples(changed_to_other_class[:,:2].squeeze(),\n",
    "                                                                   other_samples[:,:2].squeeze(),)\n",
    "            loss =  ot_loss_out + ot_transformed_in + ot_transformed_out + ot_loss_in\n",
    "            loss.backward()\n",
    "            del outs, other_class_ins, other_inverted\n",
    "            clear_ctx_dicts(feature_model)\"\"\"\n",
    "        if not gen_update:\n",
    "            optim_adv.step()\n",
    "        else:\n",
    "            optim_model.step()\n",
    "            optim_dist.step()\n",
    "    epoch_row.update({\n",
    "    'd_loss': d_loss.item(),\n",
    "    'g_loss': g_loss.item(),\n",
    "    'grad_loss': gradient_loss.item(),\n",
    "    'o_real': th.mean(score_real).item(),\n",
    "    'o_fake': th.mean(score_fake).item(),\n",
    "    'g_grad': g_grad,\n",
    "    'g_grad_norm': g_grad_norm,\n",
    "    'd_grad': d_grad,\n",
    "    'd_grad_norm': d_grad_norm,\n",
    "        'ot_out_loss': ot_out_loss.item(),\n",
    "        'ot_out_loss_other': ot_out_loss_other.item(),\n",
    "    'runtime': loop_time.elapsed_secs * 1000})\n",
    "    if i_epoch % (n_epochs // 20) != 0:\n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "        # otherwise add ot loss in\n",
    "    else:\n",
    "        for i_class in range(len(train_inputs)):\n",
    "            with th.no_grad():\n",
    "                samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 4)\n",
    "                inverted = feature_model.invert(samples)\n",
    "                clear_ctx_dicts(feature_model)\n",
    "                ot_loss_in = ot_euclidean_loss_for_samples(class_ins.view(class_ins.shape[0], -1),\n",
    "                                                           inverted.view(inverted.shape[0], -1)[:(len(class_ins))])\n",
    "                epoch_row['ot_loss_in_{:d}'.format(i_class)] = ot_loss_in.item()\n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "        print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "        print(\"G Loss: {:.2E}\".format(g_loss.item()))\n",
    "        print(\"D Loss: {:.2E}\".format(d_loss.item()))\n",
    "        print(\"OT Loss In: {:.2E}\".format(ot_loss_in.item()))\n",
    "        #print(\"OT Loss Out: {:.2E}\".format(ot_loss_out.item()))\n",
    "        #print(\"Transformed OT Loss In: {:.2E}\".format(ot_transformed_in.item()))\n",
    "        #print(\"Transformed OT Loss Out: {:.2E}\".format(ot_transformed_out.item()))\n",
    "        print(\"Loop Time: {:.0f} ms\".format(loop_time.elapsed_secs * 1000))\n",
    "        display(df.iloc[-3:])\n",
    "        plot_outs(feature_model, train_inputs, test_inputs,\n",
    "                 class_dist)\n",
    "        fig = plt.figure(figsize=(8,2))\n",
    "        plt.plot(var_to_np(th.cat((th.exp(class_dist.class_log_stds),\n",
    "                                 th.exp(class_dist.non_class_log_stds)))),\n",
    "                marker='o')\n",
    "        display_close(fig)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for n_samples_factor in [2,1]:\n",
    "        for setname, set_inputs in ((\"Train\", train_inputs), (\"Test\", test_inputs)):\n",
    "            total_loss = 0\n",
    "            total_spec_ot = 0\n",
    "            for i_class in range(2):\n",
    "                class_ins = set_inputs[i_class]\n",
    "                samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * n_samples_factor)\n",
    "                inverted = feature_model.invert(samples)\n",
    "                loss = ot_euclidean_loss_for_samples(class_ins.view(class_ins.shape[0],-1),\n",
    "                                                     inverted.view(inverted.shape[0],-1))\n",
    "                total_loss += loss\n",
    "                spec_ot = ot_euclidean_loss_for_samples(\n",
    "                    th.rfft(class_ins.squeeze(), signal_ndim=1, normalized=True).view(class_ins.shape[0],-1),\n",
    "                    th.rfft(inverted.squeeze(), signal_ndim=1, normalized=True).view(inverted.shape[0],-1))\n",
    "                total_spec_ot += spec_ot\n",
    "                clear_ctx_dicts(feature_model)\n",
    "            print(\"{:d} Samples \". format(len(samples)) + setname + \" Loss: {:.1f}\".format(total_loss.item() / 2))\n",
    "            print(\"{:d} Samples \". format(len(samples)) + setname + \" SpecLoss: {:.1f}\".format(spec_ot.item() / 2))\n",
    "\n",
    "    # Show losses between sets\n",
    "    for setname, set_inputs in ((\"Train\", train_inputs),):\n",
    "        total_loss = 0\n",
    "        total_spec_ot = 0\n",
    "        for i_class in range(2):\n",
    "            class_ins = set_inputs[i_class]\n",
    "            samples = class_dist.get_samples(i_class, len(train_inputs[i_class]) * 5) # take same number of samples\n",
    "            inverted = test_inputs[i_class]\n",
    "            loss = ot_euclidean_loss_for_samples(class_ins.view(class_ins.shape[0],-1),\n",
    "                                                     inverted.view(inverted.shape[0],-1))\n",
    "            total_loss += loss\n",
    "            spec_ot = ot_euclidean_loss_for_samples(\n",
    "                th.rfft(class_ins.squeeze(), signal_ndim=1, normalized=True).view(class_ins.shape[0],-1),\n",
    "                th.rfft(inverted.squeeze(), signal_ndim=1, normalized=True).view(inverted.shape[0],-1))\n",
    "            total_spec_ot += spec_ot\n",
    "        print(setname + \" Loss: {:.1f}\".format(total_loss.item() / 2))\n",
    "        print(setname + \" SpecLoss: {:.1f}\".format(spec_ot.item() / 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.plot import plot_head_signals_tight\n",
    "\n",
    "mean_bps_per_class = []\n",
    "for i_class in range(len(train_inputs)):\n",
    "    inverted = train_inputs[i_class]\n",
    "    mean_bps_per_class.append(\n",
    "        np.mean(np.abs(np.fft.rfft(var_to_np(inverted.squeeze()))), axis=0))\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.mean(mean_bps_per_class, axis=1).T)\n",
    "display_close(fig)\n",
    "\n",
    "fig = plot_head_signals_tight(np.stack(mean_bps_per_class, axis=-1), sensor_names=sensor_names,\n",
    "                             figsize=(20,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.plot import plot_head_signals_tight\n",
    "\n",
    "mean_bps_per_class = []\n",
    "for i_class in range(len(train_inputs)):\n",
    "    samples = class_dist.get_samples(i_class, 400)\n",
    "    inverted = feature_model.invert(samples)\n",
    "    mean_bps_per_class.append(\n",
    "        np.mean(np.abs(np.fft.rfft(var_to_np(inverted.squeeze()))), axis=0))\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.mean(mean_bps_per_class, axis=1).T)\n",
    "display_close(fig)\n",
    "\n",
    "fig = plot_head_signals_tight(np.stack(mean_bps_per_class, axis=-1), sensor_names=sensor_names,\n",
    "                             figsize=(20,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_head_signals_tight(np.log(mean_bps_per_class[0]/mean_bps_per_class[1]), sensor_names=sensor_names,\n",
    "                             figsize=(20,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    samples = class_dist.get_samples(i_class, 20)\n",
    "    inverted = feature_model.invert(samples)\n",
    "    plot_head_signals_tight(var_to_np(inverted)[0].squeeze(), sensor_names=sensor_names,\n",
    "                             figsize=(20,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df.ot_loss_in)[~np.isnan(df.ot_loss_in)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.ot_loss_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
