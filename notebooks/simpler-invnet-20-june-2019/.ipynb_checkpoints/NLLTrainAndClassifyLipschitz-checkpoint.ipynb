{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.high_gamma import load_file, create_inputs\n",
    "from reversible2.high_gamma import load_train_test\n",
    "th.backends.cudnn.benchmark = True\n",
    "from reversible2.models import deep_invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "train_inputs, test_inputs = load_train_test(\n",
    "    subject_id=4,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "test_dist_inputs, test_dist_inputs_2 = load_train_test(\n",
    "    subject_id=5,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:180,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:180,7:9].clone().contiguous() for t in test_inputs]\n",
    "test_dist_less = [t[:180,7:9].clone().contiguous() for t in test_dist_inputs]\n",
    "for t in train_less + test_less + test_dist_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.models import larger_model\n",
    "\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "\n",
    "import ot\n",
    "\n",
    "from reversible2.ot_exact import get_matched_samples\n",
    "\n",
    "\n",
    "from reversible2.model_and_dist import ModelAndDist, set_dist_to_empirical\n",
    "from reversible2.util import flatten_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.model_and_dist import create_empirical_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 201\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 1.3\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1000\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "\n",
    "            optim.step()\n",
    "            \n",
    "    if i_epoch % 10 == 0:\n",
    "        with th.no_grad():\n",
    "            samples = model_and_dist.dist.get_samples(i_class, 30).detach()\n",
    "            in_samples = model_and_dist.model.invert(samples)\n",
    "\n",
    "            out_diffs = th.norm(samples.unsqueeze(0) - samples.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            in_diffs = th.norm(flatten_2d(in_samples).unsqueeze(0) - flatten_2d(in_samples).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            out_diffs = out_diffs.flatten()\n",
    "            in_diffs = in_diffs.flatten()\n",
    "\n",
    "            assert len(out_diffs.shape) == 1\n",
    "            assert len(in_diffs.shape) == 1\n",
    "            ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "            assert len(ratio.shape) == 1\n",
    "            lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "\n",
    "            epoch_row = {'lip_loss': lip_loss.item()}\n",
    "        \n",
    "        with th.no_grad()\n",
    "        for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less), (\"Other\", test_dist_less)):\n",
    "                OTs = nlls = []\n",
    "                for i_class in range(2):\n",
    "                    examples = model_and_dist.get_examples(i_class,len(inputs[i_class]) * 20)\n",
    "                    matched_examples = get_matched_samples(flatten_2d(inputs[i_class]), flatten_2d(examples))\n",
    "                    OT = th.mean(th.norm(flatten_2d(inputs[i_class]).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                    nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                    OTs.append(OT.item())\n",
    "                    nlls.append(nll.item())\n",
    "                epoch_row[setname + '_OT'] = np.mean(OTs)\n",
    "                epoch_row[setname + '_NLL'] = np.mean(nlls)\n",
    "\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                    corrects = []\n",
    "                    for i_class in range(2):\n",
    "                        outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                        pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                        correct = pred_label == i_class\n",
    "                        corrects.extend(correct)\n",
    "                    acc = np.mean(corrects)\n",
    "                    epoch_row['model_' + setname + '_acc'] = acc\n",
    "        for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        epoch_row[name + '_' + setname + '_acc'] = acc\n",
    "                        \n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "                        \n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_csv('./old_df.csv') # save for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['Train_Test_acc', 'lip_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_normed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:30,['Train_Test_acc', 'lip_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in df_normed:\n",
    "    df_normed.loc[:,col] = df_normed.loc[:,col] - np.min(df_normed.loc[:,col])\n",
    "for col in df_normed:\n",
    "    df_normed.loc[:,col] = df_normed.loc[:,col] / np.max(df_normed.loc[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normed.loc[:, ['Train_Test_acc', 'model_Test_acc', 'lip_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mkdir /data/schirrmr/schirrmr/reversible/models/notebooks/NLLTrainAndClassifyLipschitz\n",
    "\n",
    "#th.save(model_and_dist, \"/data/schirrmr/schirrmr/reversible/models/notebooks/NLLTrainAndClassifyLipschitz/model_dist.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    train_both = th.cat(train_less, dim=0).cuda()\n",
    "    test_both = th.cat(test_less, dim=0).cuda()\n",
    "    ratios = {}\n",
    "    with th.no_grad():\n",
    "        for setname, ins in ((\"Train\", train_both), (\"Test\", test_both)):\n",
    "            outs = model_and_dist.model(ins)\n",
    "\n",
    "            out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            out_diffs = out_diffs.flatten()\n",
    "            in_diffs = in_diffs.flatten()\n",
    "\n",
    "            assert len(out_diffs.shape) == 1\n",
    "            assert len(in_diffs.shape) == 1\n",
    "            ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "            assert len(ratio.shape) == 1\n",
    "            ratios[setname] = var_to_np(ratio)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratios['Train'] = ratios['Train'][ratios['Train'] != 0]\n",
    "ratios['Test'] = ratios['Test'][ratios['Test'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seaborn.distplot(ratios['Train'])\n",
    "plt.scatter(ratios['Train'], 0 * ratios['Train'] - 0.05, alpha=0.25)\n",
    "\n",
    "seaborn.distplot(ratios['Test'])\n",
    "plt.scatter(ratios['Test'], 0 * ratios['Test'] - 0.05, alpha=0.25)\n",
    "plt.ylim(-0.075,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(1/ratios['Train'])\n",
    "plt.scatter(1/ratios['Train'], 0 * ratios['Train'] - 0.05, alpha=0.25)\n",
    "\n",
    "seaborn.distplot(1/ratios['Test'])\n",
    "plt.scatter(1/ratios['Test'], 0 * ratios['Test'] - 0.05, alpha=0.25)\n",
    "plt.ylim(-0.075,55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate copied model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "md_copy = deepcopy(model_and_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_copy.dist.class_log_stds.data.clamp_min_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "    print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "    text_strs = []\n",
    "    for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "        examples = md_copy.get_examples(1,len(inputs) * 20)\n",
    "        matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "        OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "        nll = -th.mean(md_copy.get_total_log_prob(i_class, inputs))\n",
    "        text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "        text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "    display_text(\"\\n\".join(text_strs))\n",
    "\n",
    "    print(\"Actual Model\")\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = md_copy.log_softmax(inputs[i_class].cuda())\n",
    "            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "            correct = pred_label == i_class\n",
    "            corrects.extend(correct)\n",
    "        acc = np.mean(corrects)\n",
    "        print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "    for name, inputs in ((\"Train\", train_less),\n",
    "                         (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                            test_less[i_class].cuda()), dim=0)\n",
    "                    for i_class in range(2)]),\n",
    "                         (\"Test\", test_less)):\n",
    "        emp_dist = create_empirical_dist(md_copy.model, inputs)\n",
    "\n",
    "        emp_model_dist = ModelAndDist(md_copy.model, emp_dist)\n",
    "        print(name)\n",
    "        with th.no_grad():\n",
    "            for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.append(correct)\n",
    "                acc = np.mean(np.concatenate(corrects))\n",
    "                print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                  np.mean(corrects[0]) * 100,\n",
    "                                                                  np.mean(corrects[1]) * 100))\n",
    "    examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "    matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "    fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "    for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "        ax.plot(var_to_np(signal).squeeze().T)\n",
    "        for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "            ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "            ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "    display_close(fig)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(var_to_np(th.exp(md_copy.dist.class_log_stds)[1]))\n",
    "    display_close(fig)\n",
    "    examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "    fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "    real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "    fig = plt.figure(figsize=(8,3))\n",
    "    plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "    plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "    display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun with lipschitz constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "md_copy = deepcopy(model_and_dist)\n",
    "\n",
    "optim = th.optim.Adam([{'params': md_copy.dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(md_copy.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 201\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 8\n",
    "lip_loss_factor = 1e4\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = md_copy.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "                \n",
    "                ratios = {}\n",
    "                mean_ratios =  {}\n",
    "                for setname, ins in ((\"Train\", train_less[i_class].cuda()), (\"Test\", test_less[i_class].cuda())):\n",
    "                    outs = md_copy.model(ins)\n",
    "\n",
    "                    out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                    in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                    out_diffs = out_diffs.flatten()\n",
    "                    in_diffs = in_diffs.flatten()\n",
    "\n",
    "                    assert len(out_diffs.shape) == 1\n",
    "                    assert len(in_diffs.shape) == 1\n",
    "                    ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "                    assert len(ratio.shape) == 1\n",
    "                    ratios[setname] = ratio\n",
    "                    mean_ratios[setname] = th.sum(ratio) / (len(ins) * (len(ins) - 1) / 2)\n",
    "\n",
    "                lip_loss = (F.relu(mean_ratios['Train'] - mean_ratios['Test'])) ** 2\n",
    "                \"\"\"outs = md_copy.dist.get_samples(i_class, 30).detach()\n",
    "                ins = md_copy.model.invert(outs)\n",
    "                \n",
    "                out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                out_diffs = out_diffs.flatten()\n",
    "                in_diffs = in_diffs.flatten()\n",
    "\n",
    "                assert len(out_diffs.shape) == 1\n",
    "                assert len(in_diffs.shape) == 1\n",
    "                ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "    \n",
    "                lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\"\"\"\n",
    "                lip_loss = lip_loss * lip_loss_factor\n",
    "                lip_loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Lip loss: \", lip_loss.item())\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = md_copy.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(md_copy.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = md_copy.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(md_copy.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(md_copy.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(md_copy.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "md_copy = deepcopy(model_and_dist)\n",
    "\n",
    "optim = th.optim.Adam([{'params': md_copy.dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(md_copy.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 201\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 8\n",
    "lip_loss_factor = 1e4\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = md_copy.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "                \n",
    "                outs = md_copy.model(class_ins)\n",
    "                ins = md_copy.model.invert(outs)\n",
    "                \n",
    "                out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                out_diffs = out_diffs.flatten()\n",
    "                in_diffs = in_diffs.flatten()\n",
    "\n",
    "                assert len(out_diffs.shape) == 1\n",
    "                assert len(in_diffs.shape) == 1\n",
    "                ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "    \n",
    "                lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "                lip_loss = lip_loss * lip_loss_factor\n",
    "                lip_loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Lip loss: \", lip_loss.item())\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = md_copy.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(md_copy.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = md_copy.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(md_copy.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(md_copy.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(md_copy.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = md_copy.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RFFT()\n",
    "\n",
    "ins = th.randn_like(flatten_2d(train_less[0]))#flatten_2d(test_less[0])\n",
    "outs = a(ins)\n",
    "\n",
    "out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "out_diffs = out_diffs.flatten()\n",
    "in_diffs = in_diffs.flatten()\n",
    "\n",
    "assert len(out_diffs.shape) == 1\n",
    "assert len(in_diffs.shape) == 1\n",
    "ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "th.max(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_less[0].view(-1,np.prod(train_less.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    train_both = th.cat(train_less, dim=0).cuda()\n",
    "    test_both = th.cat(test_less, dim=0).cuda()\n",
    "    ratios = {}\n",
    "    with th.no_grad():\n",
    "        for setname, ins in ((\"Train\", train_both), (\"Test\", test_both)):\n",
    "            outs = md_copy.model(ins)\n",
    "\n",
    "            out_diffs = th.norm(outs.unsqueeze(0) - outs.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            in_diffs = th.norm(flatten_2d(ins).unsqueeze(0) - flatten_2d(ins).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            out_diffs = out_diffs.flatten()\n",
    "            in_diffs = in_diffs.flatten()\n",
    "\n",
    "            assert len(out_diffs.shape) == 1\n",
    "            assert len(in_diffs.shape) == 1\n",
    "            ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "            assert len(ratio.shape) == 1\n",
    "            ratios[setname] = var_to_np(ratio)\n",
    "            \n",
    "ratios['Train'] = ratios['Train'][ratios['Train'] != 0]\n",
    "ratios['Test'] = ratios['Test'][ratios['Test'] != 0]\n",
    "\n",
    "seaborn.distplot(ratios['Train'])\n",
    "plt.scatter(ratios['Train'], 0 * ratios['Train'] - 0.05, alpha=0.25)\n",
    "\n",
    "seaborn.distplot(ratios['Test'])\n",
    "plt.scatter(ratios['Test'], 0 * ratios['Test'] - 0.05, alpha=0.25)\n",
    "plt.ylim(-0.075,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "kernel_length = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=False,\n",
    "                     kernel_length=kernel_length, constant_memory=False)\n",
    "model.cuda()\n",
    "        \n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "model_and_dist.set_dist_to_empirical(train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 5e-4}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "n_epochs = 2001\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 1.5\n",
    "lip_perturb_factor = 0.005\n",
    "lip_loss_factor = 1\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "                samples = model_and_dist.dist.get_samples(i_class, 200).detach()\n",
    "                \n",
    "                perturbations = th.rand_like(samples) - 0.5\n",
    "                norm = (lip_perturb_factor * np.sqrt(perturbations.shape[1]))\n",
    "                perturbations =  norm * (\n",
    "                    perturbations / th.norm(perturbations, p=2, dim=1, keepdim=True))\n",
    "                perturbed = samples + perturbations\n",
    "                in_perturbed = model.invert(perturbed)\n",
    "                in_samples = model.invert(samples)\n",
    "                diffs = th.norm(flatten_2d(in_samples) - flatten_2d(in_perturbed), dim=1, p=2) \n",
    "                assert len(diffs.shape) == 1\n",
    "                ratio = diffs / norm\n",
    "                lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "                lip_loss = lip_loss * lip_loss_factor\n",
    "                lip_loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.0f} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            \n",
    "            for setname, inputs in ((\"Train\", train_less[1].cuda()), (\"Test\", test_less[1].cuda()), \n",
    "                                    (\"Other\", test_dist_less[1].cuda())):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            class_ins = train_less[1].cuda()\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "kernel_length = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=False,\n",
    "                     kernel_length=kernel_length, constant_memory=False)\n",
    "model.cuda()\n",
    "        \n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "model_and_dist.set_dist_to_empirical(train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 5e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "n_epochs = 20001\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 2\n",
    "lip_loss_factor = 1\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "                \n",
    "                samples = model_and_dist.dist.get_samples(i_class, 200).detach()\n",
    "                out_a, out_b = th.chunk(samples, 2, dim=0)\n",
    "                in_a, in_b = th.chunk(model_and_dist.model.invert(samples),2,dim=0)\n",
    "                out_diffs = th.norm(out_a - out_b, dim=1, p=2) \n",
    "                assert len(out_diffs.shape) == 1\n",
    "                in_diffs = th.norm(flatten_2d(in_a) - flatten_2d(in_b), dim=1, p=2)\n",
    "                assert len(in_diffs.shape) == 1\n",
    "                ratio = in_diffs / out_diffs\n",
    "                assert len(ratio.shape) == 1\n",
    "                lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "                print(\"lip loss\", lip_loss)\n",
    "                lip_loss = lip_loss * lip_loss_factor\n",
    "                lip_loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.0f} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            \n",
    "            for setname, inputs in ((\"Train\", train_less[1].cuda()), (\"Test\", test_less[1].cuda()), \n",
    "                                    (\"Other\", test_dist_less[1].cuda())):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs,\n",
    "                                                min_std=1e-5)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            class_ins = train_less[1].cuda()\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all pairwise distances, maybe more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "n_epochs = 2001\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 2\n",
    "lip_loss_factor = 1\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "                \n",
    "                samples = model_and_dist.dist.get_samples(i_class, 25).detach()\n",
    "                in_samples = model_and_dist.model.invert(samples)\n",
    "\n",
    "                out_diffs = th.norm(samples.unsqueeze(0) - samples.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                in_diffs = th.norm(flatten_2d(in_samples).unsqueeze(0) - flatten_2d(in_samples).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "                out_diffs = out_diffs.flatten()\n",
    "                in_diffs = in_diffs.flatten()\n",
    "\n",
    "                assert len(out_diffs.shape) == 1\n",
    "                assert len(in_diffs.shape) == 1\n",
    "                ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "                assert len(ratio.shape) == 1\n",
    "                lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "                print(\"lip loss\", lip_loss)\n",
    "                lip_loss = lip_loss * lip_loss_factor\n",
    "                lip_loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.0f} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            \n",
    "            for setname, inputs in ((\"Train\", train_less[1].cuda()), (\"Test\", test_less[1].cuda()), \n",
    "                                    (\"Other\", test_dist_less[1].cuda())):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs,\n",
    "                                                min_std=1e-5)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            class_ins = train_less[1].cuda()\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = model_and_dist.dist.get_samples(i_class, 200).detach()\n",
    "\n",
    "perturbations = th.rand_like(samples) - 0.5\n",
    "# scale by stds?\n",
    "perturbations = perturbations * model_and_dist.dist.get_mean_std(i_class)[1].unsqueeze(0)\n",
    "norm = (lip_perturb_factor * np.sqrt(perturbations.shape[1])) * 1\n",
    "perturbations =  norm * (\n",
    "    perturbations / th.norm(perturbations, p=2, dim=1, keepdim=True))\n",
    "perturbed = samples + perturbations\n",
    "in_perturbed = model.invert(perturbed)\n",
    "in_samples = model.invert(samples)\n",
    "diffs = th.norm(flatten_2d(in_samples) - flatten_2d(in_perturbed), dim=1, p=2) \n",
    "assert len(diffs.shape) == 1\n",
    "ratio = diffs / norm\n",
    "lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10,3, figsize=(16,16), sharex=True, sharey=True)\n",
    "for ax, in_s, in_p in zip(axes.flatten(), var_to_np(in_samples).squeeze(), var_to_np(in_perturbed).squeeze(), ):\n",
    "    ax.plot(in_s[0])\n",
    "    ax.plot(in_p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10,3, figsize=(16,16), sharex=True, sharey=True)\n",
    "for ax, out_s, out_p in zip(axes.flatten(), var_to_np(samples).squeeze(), var_to_np(perturbed).squeeze(), ):\n",
    "    ax.plot(out_s - out_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10,3, figsize=(16,16), sharex=True, sharey=True)\n",
    "for ax, out_s, out_p in zip(axes.flatten(), var_to_np(samples).squeeze(), var_to_np(perturbed).squeeze(), ):\n",
    "    ax.plot(out_s - out_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10,3, figsize=(16,16), sharex=True, sharey=True)\n",
    "for ax, in_s, in_p in zip(axes.flatten(), var_to_np(in_samples).squeeze(), var_to_np(in_perturbed).squeeze(), ):\n",
    "    ax.plot(in_s[0])\n",
    "    ax.plot(in_p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.graph import Node\n",
    "from reversible2.rfft import RFFT\n",
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = Node(None, nn.Sequential(\n",
    "                                 ViewAs((-1, n_chans,n_time,1),(-1, n_time)),\n",
    "                                 RFFT(),\n",
    "                                ViewAs((-1, n_time), (-1, n_chans*n_time))))\n",
    "model.cuda()\n",
    "        \n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "model_and_dist.dist.class_log_stds.data.clamp_min_(0)\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "n_epochs = 501\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 1.5\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.0f} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            \n",
    "            for setname, inputs in ((\"Train\", train_less[1].cuda()), (\"Test\", test_less[1].cuda()), \n",
    "                                    (\"Other\", test_dist_less[1].cuda())):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            class_ins = train_less[1].cuda()\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
