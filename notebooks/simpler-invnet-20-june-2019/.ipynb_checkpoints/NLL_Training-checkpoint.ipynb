{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.high_gamma import load_file, create_inputs\n",
    "from reversible2.high_gamma import load_train_test\n",
    "th.backends.cudnn.benchmark = True\n",
    "from reversible2.models import deep_invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "train_inputs, test_inputs = load_train_test(\n",
    "    subject_id=4,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "test_dist_inputs, test_dist_inputs_2 = load_train_test(\n",
    "    subject_id=5,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:10,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:10,7:9].clone().contiguous() for t in test_inputs]\n",
    "test_dist_less = [t[:10,7:9].clone().contiguous() for t in test_dist_inputs]\n",
    "for t in train_less + test_less + test_dist_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.models import larger_model\n",
    "\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "\n",
    "def create_model():\n",
    "    n_chan_pad = 0\n",
    "    filter_length_time = 11\n",
    "    feature_model = deep_invertible(\n",
    "        n_chans, n_time,  n_chan_pad,  filter_length_time)\n",
    "\n",
    "    feature_model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "\n",
    "    return feature_model\n",
    "\n",
    "\n",
    "def to_generator(feature_model):\n",
    "    from reversible2.view_as import ViewAs\n",
    "    feature_model.add_module('flatten',\n",
    "                             ViewAs((-1, 8*2, 32), (-1, 8*2*32)))\n",
    "    from reversible2.graph import Node\n",
    "    feature_model = Node(None, feature_model)\n",
    "    return feature_model\n",
    "\n",
    "def create_dist():\n",
    "    return TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "    \n",
    "\n",
    "import ot\n",
    "\n",
    "from reversible2.ot_exact import get_matched_samples\n",
    "\n",
    "\n",
    "from reversible2.dist_model import ModelAndDist, set_dist_to_empirical\n",
    "from reversible2.util import flatten_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = create_dist()\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "for i_epoch in range(n_epochs):\n",
    "    log_probs = model_and_dist.get_total_log_prob(i_class, class_ins)\n",
    "    loss = -th.mean(log_probs)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        text_strs = []\n",
    "        for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "            examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "            OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "            nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "            text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "            text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "        display_text(\"\\n\".join(text_strs))\n",
    "        examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "        matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "        fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "        for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "            ax.plot(var_to_np(signal).squeeze().T)\n",
    "            for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "        display_close(fig)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "        display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:180,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:180,7:9].clone().contiguous() for t in test_inputs]\n",
    "test_dist_less = [t[:180,7:9].clone().contiguous() for t in test_dist_inputs]\n",
    "for t in train_less + test_less + test_dist_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = create_dist()\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    log_probs = model_and_dist.get_total_log_prob(i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "    loss = -th.mean(log_probs)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-np.log(noise_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strs = []\n",
    "for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "    examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "    matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "    OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "    nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "    mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "    log_probs = log_cdf_diff(model_and_dist.model(inputs), mean, std, noise_factor)\n",
    "    cdf_nll = -th.mean(log_probs)\n",
    "    text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "    text_strs.append(\"{:7s} CDF {:.1E}\".format(setname, cdf_nll.item()))\n",
    "    text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "print(\"\\n\".join(text_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = model_and_dist.get_examples(1,len(class_ins))\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "\n",
    "plt.plot(var_to_np(class_ins[:3,0].squeeze()).T, color=seaborn.color_palette()[0])\n",
    "plt.plot(var_to_np(examples[:3,0].squeeze()).T, color=seaborn.color_palette()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.fft.rfftfreq(256, 1/256.0), np.median(real_bps, axis=0))\n",
    "plt.plot(np.fft.rfftfreq(256, 1/256.0), np.median(fake_bps, axis=0))\n",
    "display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.gaussian import get_gaussian_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_strs = []\n",
    "for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "    examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "    matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "    OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "    \n",
    "    \n",
    "    nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "    outs = model_and_dist.model(inputs )\n",
    "    nll_std = get_gaussian_log_probs(\n",
    "        model_and_dist.dist.get_mean_std(i_class)[0], th.log(model_and_dist.dist.get_mean_std(i_class)[1]) * 0,\n",
    "        outs)\n",
    "    nll_std = -th.mean(nll_std)\n",
    "    text_strs.append(\"{:7s} NLL  {:8.1E}\".format(setname, nll.item()))\n",
    "    text_strs.append(\"{:7s} NLL1 {:8.1E}\".format(setname, nll_std.item()))\n",
    "    text_strs.append(\"{:7s} OT   {:8.1E}\".format(setname, OT.item()))\n",
    "print(\"\\n\".join(text_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "print(\"Train-Test:    {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(train_less[i_class]), flatten_2d(test_less[i_class]))))\n",
    "print(\"Fake-Test:     {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))\n",
    "print(\"Fake-Train:    {:.1E}\".format(ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Fake*10-Test:  {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class])*10)),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))\n",
    "print(\"Fake*10-Train: {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]) * 10)),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Zero-Train:    {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(1e-4*flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Zero-Test:     {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(1e-4*flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))\n",
    "print(\"L2 to mean:    {:.1E}\".format(\n",
    "    th.mean(th.norm(flatten_2d(class_ins) - flatten_2d(th.mean(class_ins, dim=0, keepdim=True)), dim=1, p=2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_cdf_diff(x, mean, std, eps, eps2=1e-7):\n",
    "    upper = (x + eps)\n",
    "    lower = (x - eps)\n",
    "    normed_upper = (upper - mean.unsqueeze(0)) / std.unsqueeze(0)\n",
    "    normed_lower = (lower - mean.unsqueeze(0)) / std.unsqueeze(0)\n",
    "    \n",
    "    diff = 0.5 * th.erf(normed_upper / float(np.sqrt(2))) - 0.5 * th.erf(\n",
    "        normed_lower / float(np.sqrt(2)))\n",
    "    return th.sum(th.log(diff + eps2), dim=1)\n",
    "    \n",
    "def normal_cdf(x, mean, std):\n",
    "    normed = (x - mean.unsqueeze(0)) / std.unsqueeze(0)\n",
    "    return 0.5 * (1 + th.erf(normed / float(np.sqrt(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = create_dist()\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "for i_epoch in range(n_epochs):\n",
    "    mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "    log_probs = log_cdf_diff(model_and_dist.model(class_ins), mean, std, 1e-5,)\n",
    "    loss = -th.mean(log_probs)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        text_strs = []\n",
    "        for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "            examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "            OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "            nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "            mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "            log_probs = log_cdf_diff(model_and_dist.model(inputs), mean, std, 1e-5)\n",
    "            cdf_nll = -th.mean(log_probs)\n",
    "            text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "            text_strs.append(\"{:7s} CDF {:.1E}\".format(setname, cdf_nll.item()))\n",
    "            text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "        display_text(\"\\n\".join(text_strs))\n",
    "        examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "        matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "        fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "        for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "            ax.plot(var_to_np(signal).squeeze().T)\n",
    "            for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "        display_close(fig)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "        display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.mean(th.norm(flatten_2d(class_ins) - flatten_2d(th.mean(class_ins, dim=0, keepdim=True)), dim=1, p=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
