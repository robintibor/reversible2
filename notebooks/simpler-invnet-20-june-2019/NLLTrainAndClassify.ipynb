{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.high_gamma import load_file, create_inputs\n",
    "from reversible2.high_gamma import load_train_test\n",
    "th.backends.cudnn.benchmark = True\n",
    "from reversible2.models import deep_invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "train_inputs, test_inputs = load_train_test(\n",
    "    subject_id=4,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "test_dist_inputs, test_dist_inputs_2 = load_train_test(\n",
    "    subject_id=5,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:180,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:180,7:9].clone().contiguous() for t in test_inputs]\n",
    "test_dist_less = [t[:180,7:9].clone().contiguous() for t in test_dist_inputs]\n",
    "for t in train_less + test_less + test_dist_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.models import larger_model\n",
    "\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "\n",
    "import ot\n",
    "\n",
    "from reversible2.ot_exact import get_matched_samples\n",
    "\n",
    "\n",
    "from reversible2.model_and_dist import ModelAndDist, set_dist_to_empirical\n",
    "from reversible2.util import flatten_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim.zero_grad()\n",
    "        for i_class in range(2):\n",
    "            class_ins = train_less[i_class].cuda()\n",
    "            log_probs = model_and_dist.get_total_log_prob(\n",
    "                i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "            loss = -th.mean(log_probs)\n",
    "            loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.model_and_dist import create_empirical_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    print(\"Actual Model\")\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "            correct = pred_label == i_class\n",
    "            corrects.extend(correct)\n",
    "        acc = np.mean(corrects)\n",
    "        print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "    for name, inputs in ((\"Train\", train_less),\n",
    "                         (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                            test_less[i_class].cuda()), dim=0)\n",
    "                    for i_class in range(2)]),\n",
    "                         (\"Test\", test_less)):\n",
    "        emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "        emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "        print(name)\n",
    "        with th.no_grad():\n",
    "            for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.append(correct)\n",
    "                acc = np.mean(np.concatenate(corrects))\n",
    "                print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                  np.mean(corrects[0]) * 100,\n",
    "                                                                  np.mean(corrects[1]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.distribution import TwoClassIndependentDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_stds = th.mean(th.stack((model_and_dist.dist.get_mean_std(0)[1],\n",
    "                 model_and_dist.dist.get_mean_std(1)[1]),dim=0), dim=0).clone()\n",
    "_, i_sorted = th.sort(mean_stds,descending=True)\n",
    "n_dims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for n_dims in range(1,7):\n",
    "        print(\"Class dims\", n_dims)\n",
    "        i_this_dims = i_sorted[:n_dims]\n",
    "        for name, inputs in ((\"Actual model\", None),\n",
    "                             (\"Train\", train_less),\n",
    "                             (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                test_less[i_class].cuda()), dim=0)\n",
    "                        for i_class in range(2)]),\n",
    "                             (\"Test\", test_less)):\n",
    "            this_dist = TwoClassIndependentDist(len(i_this_dims), truncate_to=None)\n",
    "            this_dist.cuda()\n",
    "            for i_class in range(2):\n",
    "                if inputs is None:\n",
    "                    mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "                else:\n",
    "                    this_outs = model_and_dist.model(inputs[i_class].cuda())\n",
    "                    mean = th.mean(this_outs, dim=0)\n",
    "                    std = th.std(this_outs, dim=0)\n",
    "                this_dist.set_mean_std(i_class, mean[i_this_dims], std[i_this_dims])\n",
    "            print(name)\n",
    "            with th.no_grad():\n",
    "                for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                    corrects = []\n",
    "                    for i_class in range(2):\n",
    "                        outs = model_and_dist.model(inner_inputs[i_class].cuda())\n",
    "                        outs = outs[:,i_this_dims]\n",
    "                        preds = this_dist.log_softmax(outs)\n",
    "                        pred_label = np.argmax(var_to_np(preds), axis=1)\n",
    "                        correct = pred_label == i_class\n",
    "                        corrects.append(correct)\n",
    "                    acc = np.mean(np.concatenate(corrects))\n",
    "                    print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                      np.mean(corrects[0]) * 100,\n",
    "                                                                      np.mean(corrects[1]) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's investigate lipschitz constant around train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = model_and_dist.model(inputs[i_class].cuda())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_avg_change = 0.01\n",
    "perturbations = th.rand_like(outs) - 0.5\n",
    "norm = (n_avg_change * np.sqrt(perturbations.shape[1]))\n",
    "perturbations =  norm * (\n",
    "    perturbations / th.norm(perturbations, dim=1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 1.3\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1000\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim.zero_grad()\n",
    "        for i_class in range(2):\n",
    "            class_ins = train_less[i_class].cuda()\n",
    "            log_probs = model_and_dist.get_total_log_prob(\n",
    "                i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "            loss = -th.mean(log_probs)\n",
    "            loss.backward()\n",
    "            \n",
    "            lip_ins = class_ins\n",
    "            outs = model_and_dist.model(lip_ins)\n",
    "            perturbations = th.rand_like(outs) - 0.5\n",
    "            norm = (lip_perturb_factor * np.sqrt(perturbations.shape[1]))\n",
    "            perturbations =  norm * (\n",
    "                perturbations / th.norm(perturbations, p=2, dim=1, keepdim=True))\n",
    "            perturbed = outs + perturbations\n",
    "            inverted = model.invert(perturbed)\n",
    "            diffs = th.norm(flatten_2d(lip_ins) - flatten_2d(inverted), dim=1, p=2) \n",
    "            ratio = diffs / norm\n",
    "            lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "            lip_loss = lip_loss * lip_loss_factor\n",
    "            lip_loss.backward()\n",
    "            \n",
    "        optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_loss / lip_loss_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm * lip_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "lip_threshold = 1.3\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1e5\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim.zero_grad()\n",
    "        for i_class in range(2):\n",
    "            class_ins = train_less[i_class].cuda()\n",
    "            log_probs = model_and_dist.get_total_log_prob(\n",
    "                i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "            loss = -th.mean(log_probs)\n",
    "            loss.backward()\n",
    "            \n",
    "            lip_ins = th.cat((class_ins, test_less[i_class].cuda()), dim=0)\n",
    "            outs = model_and_dist.model(lip_ins)\n",
    "            outs = \n",
    "            perturbations = th.rand_like(outs) - 0.5\n",
    "            norm = (lip_perturb_factor * np.sqrt(perturbations.shape[1]))\n",
    "            perturbations =  norm * (\n",
    "                perturbations / th.norm(perturbations, p=2, dim=1, keepdim=True))\n",
    "            perturbed = outs + perturbations\n",
    "            inverted = model.invert(perturbed)\n",
    "            diffs = th.norm(flatten_2d(lip_ins) - flatten_2d(inverted), dim=1, p=2) \n",
    "            ratio = diffs / norm\n",
    "            lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "            lip_loss = lip_loss * lip_loss_factor\n",
    "            lip_loss.backward()\n",
    "            \n",
    "        optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## older, retrain for test dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "optim_dist = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    test_outs = [model_and_dist.model(test_less[i_class].cuda()).detach()\n",
    "                 for i_class in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim_dist.zero_grad()\n",
    "        for i_class in range(2):\n",
    "            log_probs = model_and_dist.dist.get_total_log_prob(\n",
    "                i_class, test_outs[i_class])\n",
    "            loss = -th.mean(log_probs)\n",
    "            loss.backward()\n",
    "        optim_dist.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = log_softmax(model_and_dist, inputs[i_class].cuda())\n",
    "            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "            correct = pred_label == i_class\n",
    "            corrects.extend(correct)\n",
    "        acc = np.mean(corrects)\n",
    "        print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(model_and_dist.dist.get_mean_std(0)[1]))\n",
    "plt.plot(var_to_np(model_and_dist.dist.get_mean_std(1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.model_and_dist import ModelAndDist, create_empirical_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual Model\")\n",
    "with th.no_grad():\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = log_softmax(model_and_dist, inputs[i_class].cuda())\n",
    "            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "            correct = pred_label == i_class\n",
    "            corrects.extend(correct)\n",
    "        acc = np.mean(corrects)\n",
    "        print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "    \n",
    "\n",
    "\n",
    "for name, inputs in ((\"Train\", train_less),\n",
    "                     (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                        test_less[i_class].cuda()), dim=0)\n",
    "                for i_class in range(2)]),\n",
    "                     (\"Test\", test_less)):\n",
    "    emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "    emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "    print(name)\n",
    "    with th.no_grad():\n",
    "        for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "            corrects = []\n",
    "            for i_class in range(2):\n",
    "                outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                correct = pred_label == i_class\n",
    "                corrects.append(correct)\n",
    "            acc = np.mean(np.concatenate(corrects))\n",
    "            print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                              np.mean(corrects[0]) * 100,\n",
    "                                                              np.mean(corrects[1]) * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First reset dist to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    train_outs = [model_and_dist.model(ins.cuda()) for ins in train_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim_dist.zero_grad()\n",
    "        for i_class in range(2):\n",
    "            log_probs = model_and_dist.dist.get_total_log_prob(\n",
    "                i_class, train_outs[i_class])\n",
    "            loss = -th.mean(log_probs)\n",
    "            loss.backward()\n",
    "        optim_dist.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semi supervised now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "both_test_ins = th.cat([test_less[0].cuda(), test_less[1].cuda()], dim=0).detach()\n",
    "\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        optim.zero_grad()\n",
    "            \n",
    "        noised_ins = both_test_ins + (th.rand_like(both_test_ins) - 0.5) * noise_factor\n",
    "        log_probs_per_class = [model_and_dist.get_total_log_prob(\n",
    "                        j_class,noised_ins) for j_class in range(2)]\n",
    "\n",
    "        total_probs = th.logsumexp(th.stack(log_probs_per_class, dim=-1), dim=1)\n",
    "        loss = -th.mean(total_probs)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "        \n",
    "        with th.no_grad():\n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = log_softmax(model_and_dist, inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through number of dimensions, increasingly and use to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "both_test_ins = th.cat([test_less[0].cuda(), test_less[1].cuda()], dim=0).detach()\n",
    "\n",
    "noise_factor = 1e-2\n",
    "for i_epoch in range(n_epochs):\n",
    "    if i_epoch > 0: # skip to see starting values\n",
    "        with Timer(verbose=False) as timer:\n",
    "            optim.zero_grad()\n",
    "\n",
    "\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.rand_like(class_ins) - 0.5) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "            noised_ins = both_test_ins + (th.rand_like(both_test_ins) - 0.5) * noise_factor\n",
    "            noised_outs = model_and_dist.model(noised_ins)\n",
    "            log_probs_per_class = [model_and_dist.dist.get_total_log_prob(\n",
    "                            j_class,noised_outs) for j_class in range(2)]\n",
    "\n",
    "            total_probs = th.logsumexp(th.stack(log_probs_per_class, dim=-1), dim=1)\n",
    "            loss = -th.mean(total_probs)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.0f} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "        print(\"Actual Model\")\n",
    "        with th.no_grad():\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = log_softmax(model_and_dist, inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.extend(correct)\n",
    "                acc = np.mean(corrects)\n",
    "                print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    print(\"Actual Model\")\n",
    "    for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        corrects = []\n",
    "        for i_class in range(2):\n",
    "            outs = log_softmax(model_and_dist, inputs[i_class].cuda())\n",
    "            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "            correct = pred_label == i_class\n",
    "            corrects.extend(correct)\n",
    "        acc = np.mean(corrects)\n",
    "        print(\"{:6s} Accuracy {:.1f}\".format(setname, acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "    for name, inputs in ((\"Train\", train_less),\n",
    "                         (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                            test_less[i_class].cuda()), dim=0)\n",
    "                    for i_class in range(2)]),\n",
    "                         (\"Test\", test_less)):\n",
    "        \n",
    "        emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "        emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "        print(name)\n",
    "        with th.no_grad():\n",
    "            for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.append(correct)\n",
    "                acc = np.mean(np.concatenate(corrects))\n",
    "                print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                  np.mean(corrects[0]) * 100,\n",
    "                                                                  np.mean(corrects[1]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        for i_class in range(2):\n",
    "            outs = model_and_dist.model(inputs[i_class].cuda())\n",
    "            std = th.std(outs, dim=0)\n",
    "            plt.plot(\n",
    "                var_to_np(std).squeeze(),\n",
    "                color=seaborn.color_palette()[i_class],\n",
    "            ls={'Train':'-', 'Test':'--'}[setname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "        for i_class in range(2):\n",
    "            outs = model_and_dist.model(inputs[i_class].cuda())\n",
    "            std = th.log(th.std(outs, dim=0))\n",
    "            plt.plot(\n",
    "                var_to_np(std).squeeze(),\n",
    "                color=seaborn.color_palette()[i_class],\n",
    "            ls={'Train':'-', 'Test':'--'}[setname])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
