{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.high_gamma import load_file, create_inputs\n",
    "from reversible2.high_gamma import load_train_test\n",
    "th.backends.cudnn.benchmark = True\n",
    "from reversible2.models import deep_invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "train_inputs, test_inputs = load_train_test(\n",
    "    subject_id=4,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:10,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:10,7:9].clone().contiguous() for t in test_inputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in train_less + test_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.models import larger_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "\n",
    "def create_model():\n",
    "    n_chan_pad = 0\n",
    "    filter_length_time = 11\n",
    "    feature_model = deep_invertible(\n",
    "        n_chans, n_time,  n_chan_pad,  filter_length_time)\n",
    "    \n",
    "    feature_model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "    \n",
    "    return feature_model\n",
    "\n",
    "\n",
    "def to_generator(feature_model):\n",
    "    from reversible2.view_as import ViewAs\n",
    "    feature_model.add_module('flatten',\n",
    "                             ViewAs((-1, 8*2, 32), (-1, 8*2*32)))\n",
    "    from reversible2.graph import Node\n",
    "    feature_model = Node(None, feature_model)\n",
    "    return feature_model\n",
    "\n",
    "def create_dist():\n",
    "    return TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "    \n",
    "\n",
    "from reversible2.invert import invert\n",
    "\n",
    "class ModelAndDist(nn.Module):\n",
    "    def __init__(self, model, dist):\n",
    "        super(ModelAndDist, self).__init__()\n",
    "        self.model = model\n",
    "        self.dist = dist\n",
    "        \n",
    "    def get_examples(self, i_class, n_samples,):\n",
    "        samples = self.dist.get_samples(i_class=i_class, n_samples=n_samples)\n",
    "        if hasattr(self.model, 'invert'):\n",
    "            examples = invert(self.model, samples)\n",
    "        else:\n",
    "            examples = self.model.invert(samples)\n",
    "        return examples\n",
    "\n",
    "import ot\n",
    "\n",
    "from reversible2.ot_exact import get_matched_samples\n",
    "\n",
    "def flatten_2d(a):\n",
    "    return a.view(len(a), -1)\n",
    "\n",
    "from reversible2.constantmemory import clear_ctx_dicts\n",
    "def set_dist_to_empirical(feature_model, class_dist, inputs):\n",
    "    for i_class in range(len(inputs)):\n",
    "        with th.no_grad():\n",
    "            this_outs = feature_model(inputs[i_class].cuda())\n",
    "            mean = th.mean(this_outs, dim=0)\n",
    "            std = th.std(this_outs, dim=0)\n",
    "            class_dist.set_mean_std(i_class, mean, std)\n",
    "            # Just check\n",
    "            setted_mean, setted_std = class_dist.get_mean_std(i_class)\n",
    "            assert th.allclose(mean, setted_mean)\n",
    "            assert th.allclose(std, setted_std)\n",
    "    clear_ctx_dicts(feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "model = create_model()\n",
    "#model = to_generator(model)\n",
    "model.cuda()\n",
    "dist = create_dist()\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "\n",
    "set_dist_to_empirical(model_and_dist.model, model_and_dist.dist, train_less)\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = 1\n",
    "n_epochs = 2001\n",
    "class_ins = train_less[i_class].cuda()\n",
    "for i_epoch in range(n_epochs):\n",
    "    examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "    matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "    loss = th.mean(th.norm(flatten_2d(class_ins).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        display_text(\"OT {:.1E}\".format(loss.item()))\n",
    "        fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "        for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "            ax.plot(var_to_np(signal).squeeze().T)\n",
    "            for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "        display_close(fig)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "        display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.ot_exact import ot_euclidean_loss_for_samples\n",
    "print(\"Train-Test:    {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(train_less[i_class]), flatten_2d(test_less[i_class]))))\n",
    "print(\"Fake-Test:     {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))\n",
    "print(\"Fake-Train:    {:.1E}\".format(ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Fake*10-Test:  {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class])*10)),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))\n",
    "print(\"Fake*10-Train: {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]) * 10)),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Zero-Train:    {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(1e-4*flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(train_less[i_class].cuda()))))\n",
    "print(\"Zero-Test:     {:.1E}\".format(\n",
    "    ot_euclidean_loss_for_samples(1e-4*flatten_2d(model_and_dist.get_examples(i_class, len(train_less[i_class]))),\n",
    "                              flatten_2d(test_less[i_class].cuda()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "from reversible2.grids import create_th_grid\n",
    "def create_grid(mean, std, n_grid_points,i_dims):\n",
    "    i_dim_0, i_dim_1 = i_dims\n",
    "    mins = mean[[i_dim_0, i_dim_1]] - std[[i_dim_0, i_dim_1]]\n",
    "    maxs = mean[[i_dim_0, i_dim_1]] + std[[i_dim_0, i_dim_1]]\n",
    "    dim_0_vals = th.linspace(mins[0].item(), maxs[0].item(),\n",
    "                             n_grid_points, device=mean.device)\n",
    "    dim_1_vals = th.linspace(mins[1].item(), maxs[1].item(),\n",
    "                             n_grid_points, device=mean.device)\n",
    "\n",
    "    grid = create_th_grid(dim_0_vals, dim_1_vals)\n",
    "\n",
    "    full_grid = mean.repeat(grid.shape[0],grid.shape[1],1)\n",
    "\n",
    "    full_grid.data[:,:,i_dim_0] = grid[:,:,0]\n",
    "    full_grid.data[:,:,i_dim_1] = grid[:,:,1]\n",
    "    return full_grid, mins, maxs\n",
    "\n",
    "\n",
    "def plot_grid(inverted_grid, mins, maxs):\n",
    "    x_len, y_len = var_to_np(maxs - mins) / full_grid.shape[:-1]\n",
    "\n",
    "    max_abs = th.max(th.abs(inverted_grid))\n",
    "\n",
    "    y_factor =  (y_len / (2*max_abs)).item() * 0.9\n",
    "    fig = plt.figure(figsize=(32,32))\n",
    "\n",
    "    for i_x in range(full_grid.shape[0]):\n",
    "        for i_y in range(full_grid.shape[1]):\n",
    "            x_start = mins[0].item() + x_len * i_x + 0.1 * x_len\n",
    "            x_end = mins[0].item() + x_len * i_x + 0.9 * x_len\n",
    "            y_center = mins[1].item() + y_len * i_y + 0.5 * y_len\n",
    "\n",
    "            curve = var_to_np(inverted_grid[i_x][i_y])\n",
    "            label = ''\n",
    "            if i_x == 0 and i_y == 0:\n",
    "                label = 'Generated data'\n",
    "            plt.plot(np.linspace(x_start, x_end, len(curve)),\n",
    "                     curve * y_factor + y_center, color='black',\n",
    "                    label=label)\n",
    "    return fig\n",
    "\n",
    "def plot_two_dim_embedding(model_and_dist, i_dims, i_class, inputs=None, autoscale=False, ):\n",
    "    mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "    grid, mins, maxs = create_grid(mean, std, n_grid_points=30,i_dims=i_dims,)\n",
    "    inverted = invert(model, grid.view(-1, grid.shape[-1])).squeeze()[:,0] # ignore empty chan\n",
    "    inverted_grid = inverted.view(len(dim_0_vals), len(dim_1_vals),-1)\n",
    "    fig = plot_grid(inverted_grid, mins, maxs);\n",
    "    if inputs is not None:\n",
    "        add_outs_to_grid_plot(model_and_dist, inputs, i_dims=i_dims, autoscale=autoscale, )\n",
    "    return fig\n",
    "\n",
    "def add_outs_to_grid_plot(model_and_dist, inputs, i_dims, autoscale):\n",
    "    outs = model_and_dist.model(inputs)[:,i_dims]\n",
    "    plt.gca().set_autoscale_on(autoscale)\n",
    "    plt.scatter(var_to_np(outs[:,0]),\n",
    "               var_to_np(outs[:,1]), s=200, alpha=0.75, label='Encodings')\n",
    "\n",
    "    plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean, std = model_and_dist.dist.get_mean_std(i_class)\n",
    "i_dims = np.argsort(var_to_np(std))[::-1][:2].copy()\n",
    "fig = plot_two_dim_embedding(model_and_dist, i_dims, 1, class_ins, autoscale=False)\n",
    "fig = plot_two_dim_embedding(model_and_dist, i_dims=np.argsort(var_to_np(std))[::-1].copy()[[0,2]],\n",
    "                             i_class=1, inputs=class_ins, autoscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.graph import Node\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "from reversible2.rfft import RFFT\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from reversible2.scale import scale_to_unit_var\n",
    "from reversible2.high_gamma import load_train_test, to_signal_target\n",
    "from reversible2.scale import ScaleAndShift\n",
    "\n",
    "\n",
    "train_set, valid_set = to_signal_target(train_inputs, test_inputs)\n",
    "n_chans = train_set.X.shape[1]\n",
    "n_classes = 2\n",
    "input_time_length = train_set.X.shape[2]\n",
    "n_iters = 5\n",
    "dfs = []\n",
    "for _ in range (n_iters):\n",
    "    n_chan_pad = 0\n",
    "    filter_length_time = 11\n",
    "    model = create_model()\n",
    "    model = Node(model, nn.Sequential(\n",
    "        Expression(lambda x: x[:,:2,].unsqueeze(2)),\n",
    "        ScaleAndShift(),\n",
    "        Expression(lambda x: x.squeeze(2)),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "                    ))\n",
    "    #model.add_module(\"select_dims\", Expression(lambda x: x[:,:2,]))\n",
    "    #\n",
    "    #model.add_module(\"softmax\", nn.LogSoftmax(dim=1))\n",
    "    from reversible2.models import WrappedModel\n",
    "    model = WrappedModel(model)\n",
    "\n",
    "    model.cuda()\n",
    "    \n",
    "    for module in model.network.modules():\n",
    "        if hasattr(module, 'log_factor'):\n",
    "            module._forward_hooks.clear()\n",
    "            module.register_forward_hook(scale_to_unit_var)\n",
    "    model.network(train_inputs[0].cuda());\n",
    "    for module in model.network.modules():\n",
    "        if hasattr(module, 'log_factor'):\n",
    "            module._forward_hooks.clear()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    from copy import deepcopy\n",
    "    model_to_train = deepcopy(model)\n",
    "    lr = 1 * 1e-4\n",
    "    weight_decay = 0.5 * 1e-3\n",
    "    optimizer = AdamW(model_to_train.parameters(), lr=lr,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "    max_epochs = 50\n",
    "    model_to_train.compile(loss=F.nll_loss, optimizer=optimizer, iterator_seed=1, )\n",
    "    model_to_train.fit(train_set.X, train_set.y, epochs=max_epochs, batch_size=64,\n",
    "              scheduler='cosine',\n",
    "              validation_data=(valid_set.X, valid_set.y), )\n",
    "    dfs.append(model_to_train.epochs_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network(train_inputs[0].cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.concat([df.iloc[-1:] for df in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot real signal and matched example\n",
    "# see how they move\n",
    "# try it for another network trained on different examples and see what ot looks like then\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
