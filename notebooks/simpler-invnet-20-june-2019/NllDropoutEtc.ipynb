{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.high_gamma import load_file, create_inputs\n",
    "from reversible2.high_gamma import load_train_test\n",
    "th.backends.cudnn.benchmark = True\n",
    "from reversible2.models import deep_invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_names = ['Fz', \n",
    "                'FC3','FC1','FCz','FC2','FC4',\n",
    "                'C5','C3','C1','Cz','C2','C4','C6',\n",
    "                'CP3','CP1','CPz','CP2','CP4',\n",
    "                'P1','Pz','P2',\n",
    "                'POz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "train_inputs, test_inputs = load_train_test(\n",
    "    subject_id=4,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "# create dist\n",
    "\n",
    "test_dist_inputs, test_dist_inputs_2 = load_train_test(\n",
    "    subject_id=5,\n",
    "    car=True,\n",
    "    n_sensors=22,\n",
    "    final_hz=256,\n",
    "    start_ms=500,\n",
    "    stop_ms=1500,\n",
    "    half_before=True,\n",
    "    only_load_given_sensors=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_less = [t[:180,7:9].clone().contiguous() for t in train_inputs]\n",
    "test_less = [t[:180,7:9].clone().contiguous() for t in test_inputs]\n",
    "test_dist_less = [t[:180,7:9].clone().contiguous() for t in test_dist_inputs]\n",
    "for t in train_less + test_less + test_dist_less:\n",
    "    t.data[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.models import larger_model\n",
    "\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "\n",
    "import ot\n",
    "\n",
    "from reversible2.ot_exact import get_matched_samples\n",
    "\n",
    "\n",
    "from reversible2.model_and_dist import ModelAndDist, set_dist_to_empirical\n",
    "from reversible2.util import flatten_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "from reversible2.models import add_dropout_before_convs\n",
    "#add_dropout_before_convs(model,p_conv=0.3, p_full=0.3)\n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "model_and_dist.set_dist_to_empirical(train_less)\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.model_and_dist import create_empirical_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model_and_dist.dist.get_samples(i_class, 30).detach()\n",
    "in_samples = model_and_dist.model.invert(samples)\n",
    "perturbations = th.rand()\n",
    "\n",
    "out_diffs = th.norm(samples.unsqueeze(0) - samples.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "in_diffs = th.norm(flatten_2d(in_samples).unsqueeze(0) - flatten_2d(in_samples).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "out_diffs = out_diffs.flatten()\n",
    "in_diffs = in_diffs.flatten()\n",
    "\n",
    "assert len(out_diffs.shape) == 1\n",
    "assert len(in_diffs.shape) == 1\n",
    "ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "assert len(ratio.shape) == 1\n",
    "lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "i_class = 1\n",
    "n_epochs = 201\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1\n",
    "lip_threshold = 1.3\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1000\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.randn_like(class_ins)) * noise_factor)\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "\n",
    "            optim.step()\n",
    "            model.eval()\n",
    "            \n",
    "    if i_epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with th.no_grad():\n",
    "            samples = model_and_dist.dist.get_samples(i_class, 30).detach()\n",
    "            in_samples = model_and_dist.model.invert(samples)\n",
    "\n",
    "            out_diffs = th.norm(samples.unsqueeze(0) - samples.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            in_diffs = th.norm(flatten_2d(in_samples).unsqueeze(0) - flatten_2d(in_samples).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            out_diffs = out_diffs.flatten()\n",
    "            in_diffs = in_diffs.flatten()\n",
    "\n",
    "            assert len(out_diffs.shape) == 1\n",
    "            assert len(in_diffs.shape) == 1\n",
    "            ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "            assert len(ratio.shape) == 1\n",
    "            lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "\n",
    "            epoch_row = {'lip_loss': lip_loss.item()}\n",
    "        \n",
    "        with th.no_grad():\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less), (\"Other\", test_dist_less)):\n",
    "                    OTs = []\n",
    "                    nlls = []\n",
    "                    inputs = [i.cuda() for i in inputs]\n",
    "                    for i_class in range(2):\n",
    "                        examples = model_and_dist.get_examples(i_class,len(inputs[i_class]) * 20)\n",
    "                        matched_examples = get_matched_samples(flatten_2d(inputs[i_class]), flatten_2d(examples))\n",
    "                        OT = th.mean(th.norm(flatten_2d(inputs[i_class]).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                        nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs[i_class]))\n",
    "                        OTs.append(OT.item())\n",
    "                        nlls.append(nll.item())\n",
    "                    epoch_row[setname + '_OT'] = np.mean(OTs)\n",
    "                    epoch_row[setname + '_NLL'] = np.mean(nlls)\n",
    "\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                    corrects = []\n",
    "                    inputs =[i.cuda() for i in inputs]\n",
    "                    for i_class in range(2):\n",
    "                        outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                        pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                        correct = pred_label == i_class\n",
    "                        corrects.append(correct)\n",
    "                    acc = np.mean(np.concatenate(corrects))\n",
    "                    epoch_row['model_' + setname + '_acc'] = acc\n",
    "        for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        inner_inputs =[i.cuda() for i in inner_inputs]\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        epoch_row[name + '_' + setname + '_acc'] = acc\n",
    "                        \n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "                        \n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                inputs =  inputs.cuda()\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                inputs =[i.cuda() for i in inputs]\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.append(correct)\n",
    "                acc = np.mean(np.concatenate(corrects))\n",
    "                print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                      np.mean(corrects[0]) * 100,\n",
    "                      np.mean(corrects[1]) * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                \n",
    "                inputs =[i.cuda() for i in inputs]\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        inner_inputs =[i.cuda() for i in inner_inputs]\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split off 40 validation trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_less = [t[-40:] for t in train_less]\n",
    "train_less = [t[:-40] for t in train_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chans = train_less[0].shape[1]\n",
    "n_time = train_less[0].shape[2]\n",
    "\n",
    "\n",
    "\n",
    "n_chan_pad = 0\n",
    "filter_length_time = 11\n",
    "    \n",
    "model = larger_model(n_chans, n_time, final_fft=True, kernel_length=11, constant_memory=False)\n",
    "model.cuda()\n",
    "from reversible2.models import add_dropout_before_convs\n",
    "#add_dropout_before_convs(model,p_conv=0.3, p_full=0.3)\n",
    "dist = TwoClassIndependentDist(np.prod(train_less[0].size()[1:]))\n",
    "dist.cuda()\n",
    "model_and_dist = ModelAndDist(model, dist)\n",
    "model_and_dist.set_dist_to_empirical(train_less)\n",
    "\n",
    "optim = th.optim.Adam([{'params': dist.parameters(), 'lr':1e-2},\n",
    "                      {'params': list(model_and_dist.model.parameters()),\n",
    "                      'lr': 1e-4}])\n",
    "rand_log_noise_factor = th.zeros(1, requires_grad=True, device='cuda')\n",
    "optim_noise = th.optim.Adam([\n",
    "                       {'params': [rand_log_noise_factor], 'lr':1e-2},])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reversible2.timer import Timer\n",
    "from reversible2.distribution import TwoClassIndependentDist\n",
    "from reversible2.mixture import GaussianMixture\n",
    "n_epochs = 201\n",
    "class_ins = train_less[i_class].cuda()\n",
    "test_ins = test_less[i_class].cuda()\n",
    "test_dist_ins = test_dist_less[i_class].cuda()\n",
    "noise_factor = 1\n",
    "lip_threshold = 1.3\n",
    "lip_perturb_factor = 0.1\n",
    "lip_loss_factor = 1000\n",
    "for i_epoch in range(n_epochs):\n",
    "    with Timer(verbose=False) as timer:\n",
    "        if i_epoch > 0:\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                class_ins = train_less[i_class].cuda()\n",
    "                log_probs = model_and_dist.get_total_log_prob(\n",
    "                    i_class, class_ins + (th.randn_like(class_ins)) * th.exp(rand_log_noise_factor))\n",
    "                loss = -th.mean(log_probs)\n",
    "                loss.backward()\n",
    "\n",
    "\n",
    "            optim.step()\n",
    "            optim_noise.zero_grad()\n",
    "            for i_class in range(2):\n",
    "                train_ins = train_less[i_class].cuda()\n",
    "                valid_ins = valid_less[i_class].cuda()\n",
    "                \n",
    "                with th.no_grad():\n",
    "                    tr_outs = model_and_dist.model(train_ins).detach()\n",
    "                log_stds = rand_log_noise_factor.repeat(tr_outs.shape)\n",
    "                mixture = GaussianMixture(tr_outs, log_stds)\n",
    "                with th.no_grad():\n",
    "                    val_outs = model_and_dist.model(valid_ins).detach()\n",
    "                nll = -th.mean(mixture.log_probs(val_outs))\n",
    "                nll.backward()\n",
    "\n",
    "            optim_noise.step()\n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "            \n",
    "    if i_epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with th.no_grad():\n",
    "            samples = model_and_dist.dist.get_samples(i_class, 30).detach()\n",
    "            in_samples = model_and_dist.model.invert(samples)\n",
    "\n",
    "            out_diffs = th.norm(samples.unsqueeze(0) - samples.unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            in_diffs = th.norm(flatten_2d(in_samples).unsqueeze(0) - flatten_2d(in_samples).unsqueeze(1), p=2, dim=2)\n",
    "\n",
    "            out_diffs = out_diffs.flatten()\n",
    "            in_diffs = in_diffs.flatten()\n",
    "\n",
    "            assert len(out_diffs.shape) == 1\n",
    "            assert len(in_diffs.shape) == 1\n",
    "            ratio = in_diffs / th.clamp(out_diffs, min=1e-9)\n",
    "            assert len(ratio.shape) == 1\n",
    "            lip_loss = th.mean(F.relu(ratio - lip_threshold) ** 2)\n",
    "\n",
    "            epoch_row = {'lip_loss': lip_loss.item()}\n",
    "        \n",
    "        with th.no_grad():\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less), (\"Other\", test_dist_less)):\n",
    "                    OTs = []\n",
    "                    nlls = []\n",
    "                    inputs = [i.cuda() for i in inputs]\n",
    "                    for i_class in range(2):\n",
    "                        examples = model_and_dist.get_examples(i_class,len(inputs[i_class]) * 20)\n",
    "                        matched_examples = get_matched_samples(flatten_2d(inputs[i_class]), flatten_2d(examples))\n",
    "                        OT = th.mean(th.norm(flatten_2d(inputs[i_class]).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                        nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs[i_class]))\n",
    "                        OTs.append(OT.item())\n",
    "                        nlls.append(nll.item())\n",
    "                    epoch_row[setname + '_OT'] = np.mean(OTs)\n",
    "                    epoch_row[setname + '_NLL'] = np.mean(nlls)\n",
    "\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                    corrects = []\n",
    "                    inputs =[i.cuda() for i in inputs]\n",
    "                    for i_class in range(2):\n",
    "                        outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                        pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                        correct = pred_label == i_class\n",
    "                        corrects.append(correct)\n",
    "                    acc = np.mean(np.concatenate(corrects))\n",
    "                    epoch_row['model_' + setname + '_acc'] = acc\n",
    "        for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        inner_inputs =[i.cuda() for i in inner_inputs]\n",
    "                        corrects = []\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        epoch_row[name + '_' + setname + '_acc'] = acc\n",
    "                        \n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "                        \n",
    "\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        with th.no_grad():\n",
    "            print(\"Epoch {:d} of {:d}\".format(i_epoch, n_epochs))\n",
    "            print(\"Runtime {:.1E} ms\".format(timer.elapsed))\n",
    "            text_strs = []\n",
    "            for setname, inputs in ((\"Train\", class_ins), (\"Test\", test_ins), (\"Other\", test_dist_ins)):\n",
    "                inputs =  inputs.cuda()\n",
    "                examples = model_and_dist.get_examples(1,len(inputs) * 20)\n",
    "                matched_examples = get_matched_samples(flatten_2d(inputs), flatten_2d(examples))\n",
    "                OT = th.mean(th.norm(flatten_2d(inputs).unsqueeze(1)  - matched_examples, p=2, dim=2))#\n",
    "                nll = -th.mean(model_and_dist.get_total_log_prob(i_class, inputs))\n",
    "                text_strs.append(\"{:7s} NLL {:.1E}\".format(setname, nll.item()))\n",
    "                text_strs.append(\"{:7s} OT {:.1E}\".format(setname, OT.item()))\n",
    "            display_text(\"\\n\".join(text_strs))\n",
    "            \n",
    "            print(\"Actual Model\")\n",
    "            for setname, inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                inputs =[i.cuda() for i in inputs]\n",
    "                corrects = []\n",
    "                for i_class in range(2):\n",
    "                    outs = model_and_dist.log_softmax(inputs[i_class].cuda())\n",
    "                    pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                    correct = pred_label == i_class\n",
    "                    corrects.append(correct)\n",
    "                acc = np.mean(np.concatenate(corrects))\n",
    "                print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                      np.mean(corrects[0]) * 100,\n",
    "                      np.mean(corrects[1]) * 100))\n",
    "\n",
    "\n",
    "\n",
    "            for name, inputs in ((\"Train\", train_less),\n",
    "                                 (\"Combined\", [th.cat((train_less[i_class].cuda(),\n",
    "                                                    test_less[i_class].cuda()), dim=0)\n",
    "                            for i_class in range(2)]),\n",
    "                                 (\"Test\", test_less)):\n",
    "                \n",
    "                inputs =[i.cuda() for i in inputs]\n",
    "                emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "                emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)\n",
    "                print(name)\n",
    "                with th.no_grad():\n",
    "                    for setname, inner_inputs in ((\"Train\", train_less), (\"Test\", test_less)):\n",
    "                        corrects = []\n",
    "                        inner_inputs =[i.cuda() for i in inner_inputs]\n",
    "                        for i_class in range(2):\n",
    "                            outs = emp_model_dist.log_softmax(inner_inputs[i_class].cuda())\n",
    "                            pred_label = np.argmax(var_to_np(outs), axis=1)\n",
    "                            correct = pred_label == i_class\n",
    "                            corrects.append(correct)\n",
    "                        acc = np.mean(np.concatenate(corrects))\n",
    "                        print(\"{:6s} Accuracy {:.1f} ({:.1f}/{:.1f})\".format(setname, acc * 100,\n",
    "                                                                          np.mean(corrects[0]) * 100,\n",
    "                                                                          np.mean(corrects[1]) * 100))\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            matched_examples = get_matched_samples(flatten_2d(class_ins), flatten_2d(examples))\n",
    "            fig, axes = plt.subplots(5,2, figsize=(16,12), sharex=True, sharey=True)\n",
    "            for ax, signal, matched in zip(axes.flatten(), class_ins, matched_examples):\n",
    "                ax.plot(var_to_np(signal).squeeze().T)\n",
    "                for ex in var_to_np(matched.view(len(matched), class_ins.shape[1], class_ins.shape[2])):\n",
    "                    ax.plot(ex[0], color=seaborn.color_palette()[0], lw=0.5, alpha=0.7)\n",
    "                    ax.plot(ex[1], color=seaborn.color_palette()[1], lw=0.5, alpha=0.7)\n",
    "            display_close(fig)\n",
    "            fig = plt.figure()\n",
    "            plt.plot(var_to_np(th.exp(model_and_dist.dist.class_log_stds)[1]))\n",
    "            display_close(fig)\n",
    "            examples = model_and_dist.get_examples(1,len(class_ins) * 20)\n",
    "            fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "            real_bps = np.abs(np.fft.rfft(var_to_np(class_ins[:,0]).squeeze()))\n",
    "            fig = plt.figure(figsize=(8,3))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "            plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "            display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_model_dist = th.load('/data/schirrmr/schirrmr/reversible/experiments/dropout-weight-decay/243/model_and_dist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_and_dist.model(train_less[1][:1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ins = train_inputs[1].cuda()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(var_to_np(th.exp(other_model_dist.dist.class_log_stds)[1]))\n",
    "display_close(fig)\n",
    "examples = other_model_dist.get_examples(1,len(class_ins) * 1)\n",
    "fake_bps = np.abs(np.fft.rfft(var_to_np(examples[:,0]).squeeze()))\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(real_bps, axis=0))\n",
    "plt.plot(np.fft.rfftfreq(256, 1/256.0), np.mean(fake_bps, axis=0))\n",
    "display_close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    class_ins = train_less[i_class].cuda()\n",
    "    log_probs = model_and_dist.get_total_log_prob(\n",
    "        i_class, class_ins + (th.randn_like(class_ins)) * th.exp(rand_log_noise_factor))\n",
    "    loss = -th.mean(log_probs)\n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtures = []\n",
    "for i_class in range(2):\n",
    "    train_ins = train_less[i_class].cuda()\n",
    "    with th.no_grad():\n",
    "        tr_outs = model_and_dist.model(train_ins).detach()\n",
    "    log_stds = rand_log_noise_factor.repeat(tr_outs.shape)\n",
    "    mixture = GaussianMixture(tr_outs, log_stds)\n",
    "    mixtures.append(mixture)\n",
    "            \n",
    "\n",
    "for i_class in range(2):\n",
    "    test_ins = test_less[i_class].cuda()\n",
    "    with th.no_grad():\n",
    "        te_outs = model_and_dist.model(test_ins).detach()\n",
    "    log_probs = [m.log_probs(te_outs) for m in mixtures]\n",
    "    print(np.mean(np.argmax(var_to_np(th.stack(log_probs, dim=1)), axis=1) == i_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.exp(rand_log_noise_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    class_ins = train_less[i_class].cuda()\n",
    "    log_probs = model_and_dist.get_total_log_prob(\n",
    "        1-i_class, class_ins + (th.randn_like(class_ins)) * th.exp(rand_log_noise_factor))\n",
    "    loss = -th.mean(log_probs)\n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(dist.get_mean_std(0)[0]))\n",
    "plt.plot(var_to_np(dist.get_mean_std(1)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_out = model_and_dist.model(train_less[0].cuda())\n",
    "plt.plot(np.mean(var_to_np(tr_out), axis=0))\n",
    "tr_out = model_and_dist.model(train_less[1].cuda())\n",
    "plt.plot(np.mean(var_to_np(tr_out), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    for i_class in range(2):\n",
    "        tr_out = model_and_dist.model(train_less[i_class].cuda())\n",
    "        plt.plot(np.mean(var_to_np(tr_out), axis=0))\n",
    "    for i_class in range(2):\n",
    "        tr_out = model_and_dist.model(valid_less[i_class].cuda())\n",
    "        plt.plot(np.mean(var_to_np(tr_out), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(dist.get_mean_std(0)[1]))\n",
    "plt.plot(var_to_np(dist.get_mean_std(1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.get_mean_std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs =[i.cuda() for i in train_less]\n",
    "emp_dist = create_empirical_dist(model_and_dist.model, inputs)\n",
    "\n",
    "emp_model_dist = ModelAndDist(model_and_dist.model, emp_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(emp_model_dist.dist.get_mean_std(0)[0]))\n",
    "plt.plot(var_to_np(emp_model_dist.dist.get_mean_std(1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_model_dist.dist.get_mean_std(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p 0,0.1,0.3,0.5\n",
    "w decay 0 1e-4, 1e-3, 1e-2,\n",
    "noise factor 1e-2, 5e-2, 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
