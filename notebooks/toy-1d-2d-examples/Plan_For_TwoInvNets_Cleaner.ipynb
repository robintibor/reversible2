{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible2.sliced import sliced_from_samples\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "from reversible2.view_as import ViewAs\n",
    "from reversible2.invert import invert\n",
    "\n",
    "from reversible2.affine import AdditiveBlock\n",
    "from reversible2.plot import display_text, display_close\n",
    "from reversible2.bhno import load_file, create_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "def create_splitted_moon(n):\n",
    "    X,y  = sklearn.datasets.make_moons(n*2, shuffle=False, noise=1e-4)\n",
    "    train_inputs_a = np_to_var(X[0:n:2], dtype=np.float32)\n",
    "    train_inputs_b = np_to_var(X[1:n:2], dtype=np.float32)\n",
    "    return train_inputs_a, train_inputs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1, X2 = create_splitted_moon(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(var_to_np(X1)[:,0], var_to_np(X1)[:,1])\n",
    "plt.scatter(var_to_np(X2)[:,0], var_to_np(X2)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible2.distribution import TwoClassDist\n",
    "\n",
    "from reversible2.blocks import dense_add_block, conv_add_block_3x3\n",
    "from reversible2.rfft import RFFT, Interleave\n",
    "from reversible2.util import set_random_seeds\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "from reversible2.splitter import SubsampleSplitter\n",
    "\n",
    "\n",
    "class ModelWithDist(nn.Module):\n",
    "    def __init__(self, model, dist):\n",
    "        super(ModelWithDist, self).__init__()\n",
    "        self.model = model\n",
    "        self.dist = dist\n",
    "        \n",
    "    def \n",
    "    \n",
    "    \n",
    "def create_model():\n",
    "    feature_model = nn.Sequential(\n",
    "        dense_add_block(2,200),\n",
    "        dense_add_block(2,200),\n",
    "        dense_add_block(2,200),\n",
    "        dense_add_block(2,200),\n",
    "    )\n",
    "    class_dist = TwoClassDist(2,0, [0,1])\n",
    "    model_with_dist = ModelWithDist(feature_model, class_dist)\n",
    "    return model_with_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n1 = create_model()\n",
    "n2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = create_mixture(len(X1))\n",
    "m2 = create_mixture(len(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first train normally, see if test NLL problem appears/how strong etc.\n",
    "# expectation: points are moved towards one line, points not seen are moved away\n",
    "# -> NLL increases of not seen points\n",
    "# take second network trained on not seen data\n",
    "# expectation: symmetric effect \n",
    "# see if you evaluate kl divergence in second space, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for Xa,Xb,na,nb,ma,o_n,o_m in ((X1,X2,n1,n2,m1, op_n1,op_m1),\n",
    "                          (X2,X1,n2,n1,m2,op_n2, op_m2)) \n",
    "    # Loss between prior and mixture,\n",
    "    # Optimizing prior and model\n",
    "    oa = na(Xa)\n",
    "    mya = ma.sample(oa, detach=True)\n",
    "\n",
    "    loss = -th.mean(na.log_probs(mya))\n",
    "    o_n.zero_grad()\n",
    "    loss.backward()\n",
    "    o_n.step()\n",
    "\n",
    "    # Loss between mixture and other prior, optimizing mixture\n",
    "    with th.no_grad():\n",
    "        oa = na(Xa)\n",
    "    mya = ma.sample(oa, detach=False)\n",
    "    mya_in_b = translate(mya, na,nb)\n",
    "    loss = -th.mean(nb.log_probs(mya_in_b))\n",
    "    o_m.zero_grad()\n",
    "    loss.backward()\n",
    "    o_m.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
