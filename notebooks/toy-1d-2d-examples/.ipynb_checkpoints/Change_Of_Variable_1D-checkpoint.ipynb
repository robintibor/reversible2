{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/reversible2/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "%cd /home/schirrmr/\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible.sliced import sliced_from_samples\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "from reversible.plot import create_bw_image\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible.revnet import ResidualBlock, invert, SubsampleSplitter, ViewAs, ReversibleBlockOld\n",
    "from spectral_norm import spectral_norm\n",
    "from conv_spectral_norm import conv_spectral_norm\n",
    "\n",
    "def display_text(text, fontsize=18):\n",
    "    fig = plt.figure(figsize=(12,0.1))\n",
    "    plt.title(text, fontsize=fontsize)\n",
    "    plt.axis('off')\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_model = nn.Sequential(\n",
    "    ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        spectral_norm(nn.Linear(1,200), to_norm=0.92, n_power_iterations=3),\n",
    "             nn.ReLU(),\n",
    "             spectral_norm(nn.Linear(200,1), to_norm=0.92, n_power_iterations=3),\n",
    "    )),\n",
    "    ResidualBlock(\n",
    "            nn.Sequential(\n",
    "            spectral_norm(nn.Linear(1,200), to_norm=0.92, n_power_iterations=3),\n",
    "                 nn.ReLU(),\n",
    "                 spectral_norm(nn.Linear(200,1), to_norm=0.92, n_power_iterations=3),\n",
    "        )),)\n",
    "optimizer = th.optim.Adam(feature_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check that gradient is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "grad_all = autograd.grad(outputs=outs, inputs=examples,\n",
    "                       grad_outputs=th.ones(examples.size()),\n",
    "                       create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "grad_per_part = th.cat([autograd.grad(outputs=outs[i_example], inputs=examples,\n",
    "                       create_graph=True, retain_graph=True)[0][i_example] for i_example in range(300)])\n",
    "\n",
    "assert np.all(var_to_np(grad_all.squeeze() == grad_per_part) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5001\n",
    "for i_epoch in range(n_epochs):\n",
    "    examples = (th.randn(300,1) * 3 + 2).requires_grad_(True)\n",
    "\n",
    "\n",
    "    outs = feature_model(examples)\n",
    "\n",
    "    grad_all = autograd.grad(outputs=outs, inputs=examples,\n",
    "                           grad_outputs=th.ones(examples.size()),\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "    loss = -th.sum(th.log(grad_all)) + th.sum(outs * outs)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Epoch {:d} of {:d}\".format(i_epoch,n_epochs))\n",
    "        print(\"Loss: {:.2f}\".format(loss.item()))\n",
    "        \n",
    "        fig = plt.figure(figsize=(5,2))\n",
    "        plt.plot(var_to_np(outs)[:,0], var_to_np(outs[:,0] * 0), marker='o', ls='')\n",
    "        display(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(var_to_np(outs).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniform dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_model = nn.Sequential(\n",
    "    ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        spectral_norm(nn.Linear(1,200), to_norm=0.92, n_power_iterations=3),\n",
    "             nn.ReLU(),\n",
    "             spectral_norm(nn.Linear(200,1), to_norm=0.92, n_power_iterations=3),\n",
    "    )),\n",
    "    ResidualBlock(\n",
    "            nn.Sequential(\n",
    "            spectral_norm(nn.Linear(1,200), to_norm=0.92, n_power_iterations=3),\n",
    "                 nn.ReLU(),\n",
    "                 spectral_norm(nn.Linear(200,1), to_norm=0.92, n_power_iterations=3),\n",
    "        )),)\n",
    "optimizer = th.optim.Adam(feature_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5001\n",
    "for i_epoch in range(n_epochs):\n",
    "    examples = (th.rand(300,1) * 3 + 1).requires_grad_(True)\n",
    "\n",
    "\n",
    "    outs = feature_model(examples)\n",
    "\n",
    "    grad_all = autograd.grad(outputs=outs, inputs=examples,\n",
    "                           grad_outputs=th.ones(examples.size()),\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "    loss = -th.sum(th.log(grad_all)) + th.sum(outs * outs)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Epoch {:d} of {:d}\".format(i_epoch,n_epochs))\n",
    "        print(\"Loss: {:.2f}\".format(loss.item()))\n",
    "        \n",
    "        fig = plt.figure(figsize=(5,2))\n",
    "        plt.plot(var_to_np(outs)[:,0], var_to_np(outs[:,0] * 0), marker='o', ls='')\n",
    "        display(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(var_to_np(outs).squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
