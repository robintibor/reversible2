{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/reversible2/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "%cd /home/schirrmr/\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible.sliced import sliced_from_samples\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "from reversible.plot import create_bw_image\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible.revnet import ResidualBlock, invert, SubsampleSplitter, ViewAs, ReversibleBlockOld\n",
    "from spectral_norm import spectral_norm\n",
    "from conv_spectral_norm import conv_spectral_norm\n",
    "\n",
    "def display_text(text, fontsize=18):\n",
    "    fig = plt.figure(figsize=(12,0.1))\n",
    "    plt.title(text, fontsize=fontsize)\n",
    "    plt.axis('off')\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.bbci import BBCIDataset\n",
    "from braindecode.mne_ext.signalproc import mne_apply\n",
    "# we loaded all sensors to always get same cleaning results independent of sensor selection\n",
    "# There is an inbuilt heuristic that tries to use only EEG channels and that definitely\n",
    "# works for datasets in our paper\n",
    "#train_loader = BBCIDataset('/data/schirrmr/schirrmr/HGD-public/reduced/train/13.mat')\n",
    "#test_loader = BBCIDataset('/data/schirrmr/schirrmr/HGD-public/reduced/test/13.mat')\n",
    "start_cnt = BBCIDataset('/data/schirrmr/schirrmr/HGD-public/reduced/train/4.mat',).load()\n",
    "start_cnt = start_cnt.drop_channels(['STI 014'])\n",
    "def car(a):\n",
    "    return a - np.mean(a, keepdims=True, axis=0)\n",
    "\n",
    "start_cnt = mne_apply(\n",
    "    car, start_cnt)\n",
    "\n",
    "start_cnt = start_cnt.reorder_channels(['C3', 'C4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne\n",
    "\n",
    "marker_def = OrderedDict([('Right Hand', [1]), ('Left Hand', [2],),\n",
    "                         ('Rest', [3]), ('Feet', [4])])\n",
    "ival = [500,1500]\n",
    "from braindecode.mne_ext.signalproc import mne_apply, resample_cnt\n",
    "from braindecode.datautil.signalproc import exponential_running_standardize, bandpass_cnt\n",
    "\n",
    "log.info(\"Resampling train...\")\n",
    "cnt = resample_cnt(start_cnt, 250.0)\n",
    "log.info(\"Standardizing train...\")\n",
    "cnt = mne_apply(lambda a: exponential_running_standardize(a.T ,factor_new=1e-3, init_block_size=1000, eps=1e-4).T,\n",
    "                     cnt)\n",
    "cnt = resample_cnt(cnt, 32.0)\n",
    "cnt = resample_cnt(cnt, 64.0)\n",
    "#cnt = mne_apply(\n",
    "#    lambda a: bandpass_cnt(a, 0, 2, cnt.info['sfreq'],\n",
    "#                           filt_order=10,\n",
    "#                           axis=1), cnt)\n",
    "\n",
    "train_set = create_signal_target_from_raw_mne(cnt, marker_def, ival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_right = train_set.X[train_set.y == 0]\n",
    "\n",
    "x_rest = train_set.X[train_set.y == 2]\n",
    "\n",
    "inputs_a = np_to_var(x_right[:160,0:1,:,None], dtype=np.float32).cuda()\n",
    "\n",
    "inputs_b = np_to_var(x_rest[:160,0:1,:,None], dtype=np.float32).cuda()\n",
    "inputs = [inputs_a, inputs_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "for i_class in range(2):\n",
    "    ins = var_to_np(inputs[i_class].squeeze())\n",
    "    bps = np.abs(np.fft.rfft(ins.squeeze()))\n",
    "    plt.plot(np.fft.rfftfreq(ins.squeeze().shape[1], d=1/ins.squeeze().shape[1]), np.median(bps, axis=0))\n",
    "\n",
    "    \n",
    "plt.title(\"Spectrum\")\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(['Real Right', 'Fake Right', 'Real Rest', 'Fake Rest'])\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(16,20, figsize=(14,14), sharex=True, sharey=True)\n",
    "for i_class in range(2):\n",
    "    for i_example in range(len(inputs[i_class])):\n",
    "        i_row = i_example // 10\n",
    "        i_col = i_example % 10\n",
    "        i_col += i_class * 10\n",
    "        axes[i_row][i_col].plot(var_to_np(inputs[i_class][i_example]).squeeze(),\n",
    "                               color=seaborn.color_palette()[i_class])\n",
    "fig.suptitle('Input signals')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], color=seaborn.color_palette()[i_class],) for i_class in range(2)]\n",
    "labels = ['Right', 'Rest',]\n",
    "axes[0][-1].legend(lines, labels, bbox_to_anchor=(1,1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean shift: first layer mean\n",
    "# strength of 1-hz power\n",
    "# phase\n",
    "\n",
    "# first two layers, then remove mean and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    ins = var_to_np(inputs[i_class].squeeze())\n",
    "    bps = np.abs(np.fft.rfft(ins.squeeze()))\n",
    "    angles = np.angle((np.fft.rfft(ins.squeeze())))\n",
    "    for i_bp in range(bps.shape[1]):\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        seaborn.distplot(bps[:,i_bp], color=seaborn.color_palette()[i_class], bins=80, rug=True)\n",
    "        plt.title(\"Amplitudes\")\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        seaborn.distplot(angles[:,i_bp] % (2*np.pi), color=seaborn.color_palette()[i_class], bins=80, rug=True)\n",
    "        plt.title(\"Phase\")\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class = 1\n",
    "i_bp = 18\n",
    "ins = var_to_np(inputs[i_class].squeeze())\n",
    "bps = np.abs(np.fft.rfft(ins.squeeze()))\n",
    "angles = np.angle((np.fft.rfft(ins.squeeze())))\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "seaborn.distplot(angles[:,i_bp] % (2*np.pi), color=seaborn.color_palette()[i_class], bins=80, rug=True)\n",
    "plt.title(\"Phase\")\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible.uniform import get_uniform_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_bp = 18\n",
    "points = np_to_var(angles[:,i_bp], dtype=np.float32)\n",
    "sorted_points = th.sort(points, dim=0)[0].detach()\n",
    "\n",
    "weights = th.randn(200, requires_grad=True)\n",
    "bias = th.randn(200, requires_grad=True)\n",
    "alphas = th.randn(200, requires_grad=True)\n",
    "#weights.data *= 0.01\n",
    "#bias.data *= 3\n",
    "\n",
    "\n",
    "def leaky_relu_upper_bounded(x, negative_slope=0.1, max_val=1):\n",
    "    return th.clamp(nn.functional.leaky_relu(x, negative_slope), min=-1000, max=max_val)\n",
    "\n",
    "def inv_sigmoid(p, eps=1e-8):\n",
    "    return th.log(p + eps) - th.log(1-p + eps)\n",
    "\n",
    "samples = th.linspace(-3,3).unsqueeze(1)\n",
    "out = inv_sigmoid(th.mean(th.sigmoid(alphas.unsqueeze(0)) * th.sigmoid(samples *\n",
    "                                   th.exp(weights).unsqueeze(0) \n",
    "                                  + bias.unsqueeze(0)), dim=1))\n",
    "\n",
    "# 0.43 without transformation\n",
    "mean = th.zeros(1, requires_grad=True)\n",
    "log_std = th.zeros(1, requires_grad=True)\n",
    "optim = th.optim.Adam([mean,log_std, weights, bias, alphas], lr=1e-1)\n",
    "\n",
    "n_epochs = 100\n",
    "for i_epoch in range(n_epochs):\n",
    "    std = th.exp(log_std)\n",
    "\n",
    "    samples = get_uniform_samples(len(angles) * 3, mean, std * 2 * np.pi)\n",
    "    transformed_samples = inv_sigmoid(th.mean(th.sigmoid(alphas.unsqueeze(0)) * th.sigmoid(samples *\n",
    "                                   th.exp(weights).unsqueeze(0) \n",
    "                                  + bias.unsqueeze(0)), dim=1))\n",
    "    sorted_samples, _ = th.sort(transformed_samples, dim=0)\n",
    "\n",
    "    diffs = sorted_samples.reshape(len(points), -1) - sorted_points.unsqueeze(1)\n",
    "    loss = th.sqrt(th.mean(diffs * diffs))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Loss: {:.2f}\".format(loss.item()))\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "        seaborn.distplot(var_to_np(sorted_points), color=seaborn.color_palette()[0], bins=80, rug=True)\n",
    "        seaborn.distplot(var_to_np(sorted_samples), color=seaborn.color_palette()[1], bins=80, rug=True)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        orig_sorted_samples = th.sort(samples, dim=0)[0]\n",
    "        transformed_samples = inv_sigmoid(th.mean(th.sigmoid(alphas.unsqueeze(0)) * th.sigmoid(orig_sorted_samples *\n",
    "                                   th.exp(weights).unsqueeze(0) \n",
    "                                  + bias.unsqueeze(0)), dim=1))\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        plt.plot(var_to_np(orig_sorted_samples), var_to_np(transformed_samples))\n",
    "        display(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_bp = 18\n",
    "points = np_to_var(angles[:,i_bp], dtype=np.float32)\n",
    "sorted_points = th.sort(points, dim=0)[0].detach()\n",
    "\n",
    "weights = th.randn(200, requires_grad=True)\n",
    "bias = th.randn(200, requires_grad=True)\n",
    "bounds_a = th.randn(200, requires_grad=True)\n",
    "bounds_b = th.randn(200, requires_grad=True)\n",
    "#weights.data *= 0.01\n",
    "#bias.data *= 3\n",
    "\n",
    "\n",
    "def leaky_bounded_relu(x,  lower_bounds, upper_bounds,leak_slope=0.01,):\n",
    "    return x * leak_slope + th.min(th.max(x, lower_bounds.unsqueeze(0)), upper_bounds.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = samples * th.exp(weights).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = th.min(bounds_a, bounds_b)\n",
    "upper_bounds = th.max(bounds_a, bounds_b)\n",
    "\n",
    "leaky_bounded_relu(a, lower_bounds, upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = th.linspace(-3,3).unsqueeze(1)\n",
    "out = th.mean(leaky_bounded_relu(\n",
    "    samples * th.exp(weights).unsqueeze(0) + bias.unsqueeze(0),\n",
    "    lower_bounds, upper_bounds), dim=1)\n",
    "plt.plot(var_to_np(samples), var_to_np(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.max(th.exp(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.43 without transformation\n",
    "mean = th.zeros(1, requires_grad=True)\n",
    "log_std = th.zeros(1, requires_grad=True)\n",
    "optim = th.optim.Adam([mean,log_std, weights, bias, bounds_a, bounds_b], lr=1e-1)\n",
    "\n",
    "n_epochs = 100\n",
    "for i_epoch in range(n_epochs):\n",
    "    std = th.exp(log_std)\n",
    "    samples = get_uniform_samples(len(angles) * 3, mean, std * 2 * np.pi)\n",
    "    lower_bounds = th.min(bounds_a, bounds_b)\n",
    "    upper_bounds = th.max(bounds_a, bounds_b)\n",
    "    transformed_samples = th.mean(leaky_bounded_relu(\n",
    "            samples * th.exp(weights).unsqueeze(0) + bias.unsqueeze(0),\n",
    "            lower_bounds, upper_bounds), dim=1)\n",
    "    sorted_samples, _ = th.sort(transformed_samples, dim=0)\n",
    "\n",
    "    diffs = sorted_samples.reshape(len(points), -1) - sorted_points.unsqueeze(1)\n",
    "    loss = th.sqrt(th.mean(diffs * diffs))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Loss: {:.2f}\".format(loss.item()))\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "        seaborn.distplot(var_to_np(sorted_points), color=seaborn.color_palette()[0], bins=80, rug=True)\n",
    "        seaborn.distplot(var_to_np(sorted_samples), color=seaborn.color_palette()[1], bins=80, rug=True)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        orig_sorted_samples = th.sort(samples, dim=0)[0]\n",
    "        transformed_samples = th.mean(leaky_bounded_relu(\n",
    "            orig_sorted_samples * th.exp(weights).unsqueeze(0) + bias.unsqueeze(0),\n",
    "            lower_bounds, upper_bounds), dim=1)\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        plt.plot(var_to_np(orig_sorted_samples), var_to_np(transformed_samples))\n",
    "        display(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_bp = 18\n",
    "points = np_to_var(angles[:,i_bp], dtype=np.float32)\n",
    "sorted_points = th.sort(points, dim=0)[0].detach()\n",
    "\n",
    "weights = th.randn(200, requires_grad=True)\n",
    "bias = th.randn(200, requires_grad=True)\n",
    "bounds_a = th.randn(200, requires_grad=True)\n",
    "bounds_b = th.randn(200, requires_grad=True)\n",
    "#weights.data *= 0.01\n",
    "#bias.data *= 3\n",
    "\n",
    "\n",
    "def leaky_bounded_relu(x,  lower_bounds, upper_bounds,leak_slope=0.01,):\n",
    "    return x * leak_slope + th.min(th.max(x, lower_bounds.unsqueeze(0)), upper_bounds.unsqueeze(0))\n",
    "# 0.43 without transformation\n",
    "mean = th.zeros(1, requires_grad=True)\n",
    "log_std = th.zeros(1, requires_grad=True)\n",
    "optim = th.optim.Adam([mean,log_std, weights, bias, bounds_a, bounds_b], lr=1e-1)\n",
    "\n",
    "n_epochs = 100\n",
    "for i_epoch in range(n_epochs):\n",
    "    std = th.exp(log_std)\n",
    "    samples = get_uniform_samples(len(angles) * 3, mean, std * 2 * np.pi)\n",
    "    lower_bounds = th.min(bounds_a, bounds_b)\n",
    "    upper_bounds = th.max(bounds_a, bounds_b)\n",
    "    transformed_samples = th.mean(leaky_bounded_relu(\n",
    "            samples * th.exp(weights).unsqueeze(0) + bias.unsqueeze(0),\n",
    "            lower_bounds, upper_bounds), dim=1)\n",
    "    sorted_samples, _ = th.sort(transformed_samples, dim=0)\n",
    "\n",
    "    diffs = sorted_samples.reshape(len(points), -1) - sorted_points.unsqueeze(1)\n",
    "    loss = th.sqrt(th.mean(diffs * diffs))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i_epoch % (n_epochs // 20) == 0:\n",
    "        print(\"Loss: {:.2f}\".format(loss.item()))\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "        seaborn.distplot(var_to_np(sorted_points), color=seaborn.color_palette()[0], bins=80, rug=True)\n",
    "        seaborn.distplot(var_to_np(sorted_samples), color=seaborn.color_palette()[1], bins=80, rug=True)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        orig_sorted_samples = th.sort(samples, dim=0)[0]\n",
    "        transformed_samples = th.mean(leaky_bounded_relu(\n",
    "            orig_sorted_samples * th.exp(weights).unsqueeze(0) + bias.unsqueeze(0),\n",
    "            lower_bounds, upper_bounds), dim=1)\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        plt.plot(var_to_np(orig_sorted_samples), var_to_np(transformed_samples))\n",
    "        display(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_samples, _ = th.sort(transformed_samples, dim=0)\n",
    "    samples = get_uniform_samples(len(angles) * 3, mean, std * 2 * np.pi)\n",
    "    transformed_samples = th.mean(th.exp(alphas.unsqueeze(0)) * th.sigmoid(samples *\n",
    "                                   th.exp(weights).unsqueeze(0) \n",
    "                                  + bias.unsqueeze(0)), dim=1)\n",
    "seaborn.distplot(var_to_np(sorted_points), color=seaborn.color_palette()[0], bins=80, rug=True)\n",
    "seaborn.distplot(var_to_np(sorted_samples), color=seaborn.color_palette()[1], bins=80, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.exp(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = th.linspace(-3,3).unsqueeze(1)\n",
    "out = th.mean(th.nn.functional.leaky_relu(samples *\n",
    "                                   th.exp(weights).unsqueeze(0) \n",
    "                                  + bias.unsqueeze(0)), dim=1)\n",
    "plt.plot(var_to_np(a), var_to_np(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-np.pi +0.14) % (2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #plt.plot(np.fft.rfftfreq(ins.squeeze().shape[1], d=1/ins.squeeze().shape[1]), np.median(bps, axis=0))\n",
    "\n",
    "    \n",
    "plt.title(\"Spectrum\")\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(['Real Right', 'Fake Right', 'Real Rest', 'Fake Rest'])\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.figure(figsize=(10,6))\n",
    "for i_class in range(2):\n",
    "    plt.plot(var_to_np(inputs[i_class].squeeze()).T, color=seaborn.color_palette()[i_class],lw=0.5);\n",
    "lines = [Line2D([0], [0], color=seaborn.color_palette()[i_class],) for i_class in range(2)]\n",
    "plt.legend(lines, ['Right', 'Rest',], bbox_to_anchor=(1,1,0,0))\n",
    "plt.title('Input signals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_block(n_c, n_i_c):\n",
    "     return ReversibleBlockOld(\n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c // 2, n_i_c,(3,1), stride=1, padding=(1,0),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c // 2,(3,1), stride=1, padding=(1,0),bias=True)),\n",
    "         \n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c // 2, n_i_c,(3,1), stride=1, padding=(1,0),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c // 2,(3,1), stride=1, padding=(1,0),bias=True))\n",
    "    )\n",
    "    \n",
    "def res_block(n_c, n_i_c):\n",
    "     return ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c, n_i_c, (3,1), stride=1, padding=(1,0),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c, (3,1), stride=1, padding=(1,0),bias=True)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminator import ProjectionDiscriminator\n",
    "from reversible.revnet import SubsampleSplitter, ViewAs\n",
    "from reversible.util import set_random_seeds\n",
    "from reversible.revnet import init_model_params\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "from conv_spectral_norm import conv_spectral_norm\n",
    "set_random_seeds(2019011641, True)\n",
    "feature_model = nn.Sequential(\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(2,32),\n",
    "    rev_block(2,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(4,32),\n",
    "    rev_block(4,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(8,32),\n",
    "    rev_block(8,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(16,32),\n",
    "    rev_block(16,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(32,32),\n",
    "    rev_block(32,32),\n",
    "    SubsampleSplitter(stride=[2,1],chunk_chans_first=False),\n",
    "    rev_block(64,64),\n",
    "    rev_block(64,64),\n",
    "    ViewAs((-1,64,1,1), (-1,64))\n",
    ")\n",
    "feature_model.cuda()\n",
    "\n",
    "\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "adv_model = nn.Sequential(\n",
    "    nn.Conv2d(1,16, (3,1), stride=1, padding=(1,0),bias=True),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,1)),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,1)),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,1)),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,1)),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,1)),\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    ViewAs((-1,16,2,1), (-1,32)),\n",
    "    )\n",
    "\n",
    "adv_model = ProjectionDiscriminator(adv_model,32,2,)\n",
    "\n",
    "adv_model.cuda()\n",
    "\n",
    "\n",
    "from reversible.training import hard_init_std_mean\n",
    "n_dims = inputs_b.shape[2]\n",
    "n_clusters = 2\n",
    "means_per_cluster = [th.autograd.Variable(th.ones(n_dims).cuda(), requires_grad=True)\n",
    "                     for _ in range(n_clusters)]\n",
    "# keep in mind this is in log domain so 0 is std 1\n",
    "stds_per_cluster = [th.autograd.Variable(th.zeros(n_dims).cuda(), requires_grad=True)\n",
    "                    for _ in range(n_clusters)]\n",
    "\n",
    "\n",
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class])\n",
    "    means_per_cluster[i_class].data = th.mean(this_outs, dim=0).data\n",
    "    stds_per_cluster[i_class].data = th.log(th.std(this_outs, dim=0)).data\n",
    "    # override phase\n",
    "    means_per_cluster[i_class].data[len(stds_per_cluster[i_class])//2:] = 0\n",
    "    stds_per_cluster[i_class].data[len(stds_per_cluster[i_class])//2:] = 0\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "optimizer = th.optim.Adam(\n",
    "                          [\n",
    "    {'params': list(feature_model.parameters()),\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 0},], betas=(0,0.9))\n",
    "\n",
    "optim_dist = th.optim.Adam(\n",
    "                          [\n",
    "    {'params': means_per_cluster + stds_per_cluster,\n",
    "    'lr': 1e-2,\n",
    "    'weight_decay': 0},], betas=(0,0.9))\n",
    "\n",
    "\n",
    "optim_adv = th.optim.Adam([{\n",
    "    'params': adv_model.parameters(),\n",
    "    'lr': 4e-3, 'weight_decay': 0.00}],#lr 0.0004\n",
    "                         betas=(0,0.9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
