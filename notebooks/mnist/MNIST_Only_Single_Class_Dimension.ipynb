{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/reversible/reversible2/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/explaining/reversible//')\n",
    "%cd /home/schirrmr/\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "from reversible.sliced import sliced_from_samples\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "from reversible.plot import create_bw_image\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from reversible.revnet import ResidualBlock, invert, SubsampleSplitter, ViewAs, ReversibleBlockOld\n",
    "from spectral_norm import spectral_norm\n",
    "from conv_spectral_norm import conv_spectral_norm\n",
    "\n",
    "def display_text(text, fontsize=18):\n",
    "    fig = plt.figure(figsize=(12,0.1))\n",
    "    plt.title(text, fontsize=fontsize)\n",
    "    plt.axis('off')\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "## from  http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "train, val, test = pickle.load(gzip.open('/data/schirrmr/schirrmr/mnist/mnist.pkl.gz'), encoding='bytes')\n",
    "\n",
    "X_train, y_train = train\n",
    "X_val, y_val = val\n",
    "\n",
    "X_train_topo = X_train.reshape(X_train.shape[0], 1, 28,28)\n",
    "X_val_topo = X_val.reshape(X_val.shape[0], 1, 28,28)\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible.util import np_to_var, var_to_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0 = X_train_topo[y_train == 0][:50]\n",
    "x_1 = X_train_topo[y_train == 1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [np_to_var(x_0, dtype=np.float32).cuda(),\n",
    "          np_to_var(x_1, dtype=np.float32).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    image_grid = np.zeros((5,10,28,28))\n",
    "    for i_row in range(image_grid.shape[0]):\n",
    "        for i_col in range(image_grid.shape[1]):\n",
    "            image_grid[i_row, i_col] = var_to_np(inputs[i_class][i_row * image_grid.shape[1] + i_col])\n",
    "    im = create_bw_image(image_grid).resize((image_grid.shape[1]*100,image_grid.shape[0]*100))\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible.revnet import ReversibleBlockOld\n",
    "import torch.nn as nn\n",
    "\n",
    "def rev_block(n_chans, n_intermediate_chans, kernel_size=3):\n",
    "    c = n_chans // 2\n",
    "    n_i_c = n_intermediate_chans\n",
    "    assert kernel_size % 2 == 1\n",
    "    padding = kernel_size // 2\n",
    "    return ReversibleBlockOld(\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c, n_i_c, (kernel_size, kernel_size), padding=padding),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(n_i_c, c, (kernel_size,kernel_size), padding=padding)),\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c, n_i_c, (kernel_size,kernel_size), padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, c, (kernel_size,kernel_size), padding=padding)))\n",
    "def res_block(n_c, n_i_c):\n",
    "     return ResidualBlock(\n",
    "        nn.Sequential(\n",
    "        nn.Conv2d(n_c, n_i_c, (3,3), stride=1, padding=(1,1),bias=True),\n",
    "        nn.ReLU(),\n",
    "            nn.Conv2d(n_i_c, n_c, (3,3), stride=1, padding=(1,1),bias=True)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible.revnet import SubsampleSplitter, ViewAs\n",
    "from reversible.util import set_random_seeds\n",
    "from reversible.revnet import init_model_params\n",
    "from torch.nn import ConstantPad2d\n",
    "import torch as th\n",
    "set_random_seeds(3049, True)\n",
    "feature_model = th.nn.Sequential(\n",
    "    ConstantPad2d((2,2,2,2), 0),\n",
    "    SubsampleSplitter(stride=2,checkerboard=True, chunk_chans_first=False),\n",
    "    rev_block(4,25),\n",
    "    rev_block(4,25),\n",
    "    SubsampleSplitter(stride=2,checkerboard=True),\n",
    "    rev_block(16,50),\n",
    "    rev_block(16,50),\n",
    "    SubsampleSplitter(stride=2,checkerboard=True),\n",
    "    rev_block(64,100),\n",
    "    rev_block(64,100),\n",
    "    SubsampleSplitter(stride=2,checkerboard=True),\n",
    "    rev_block(256,200),\n",
    "    rev_block(256,200),\n",
    "    SubsampleSplitter(stride=2,checkerboard=True),\n",
    "    rev_block(1024,200, kernel_size=1),\n",
    "    ViewAs((-1,1024,1,1),(-1,1024)),)\n",
    "\n",
    "feature_model = feature_model.cuda()\n",
    "\n",
    "from discriminator import ProjectionDiscriminator\n",
    "\n",
    "from reversible.training import hard_init_std_mean\n",
    "n_dims = 1024\n",
    "n_clusters = 2\n",
    "means_per_cluster = [th.autograd.Variable(th.zeros(n_dims).cuda(), requires_grad=True)\n",
    "                     for _ in range(n_clusters)]\n",
    "\n",
    "\n",
    "stds_per_cluster = [th.autograd.Variable(th.zeros(n_dims).cuda(), requires_grad=True)\n",
    "                    for _ in range(n_clusters)]\n",
    "\n",
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class])\n",
    "    means_per_cluster[i_class].data = th.mean(this_outs, dim=0).data\n",
    "    stds_per_cluster[i_class].data = th.log(th.std(this_outs, dim=0)).data\n",
    "\n",
    "from copy import deepcopy\n",
    "optimizer = th.optim.Adam(\n",
    "                          [\n",
    "    {'params': list(feature_model.parameters()),\n",
    "    'lr': 0.0001,\n",
    "    'weight_decay': 0},], betas=(0,0.9))\n",
    "\n",
    "\n",
    "adv_model = nn.Sequential(\n",
    "    nn.Conv2d(1,16, (3,3), stride=1, padding=(1,1),bias=True),#28\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,2)),#14\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,2)),#7\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,2), stride=(1,1)),#6\n",
    "    res_block(16,32),\n",
    "    res_block(16,32),\n",
    "    nn.AvgPool2d((2,2)),\n",
    "    ViewAs((-1,16,3,3), (-1,144)),\n",
    "    )\n",
    "\n",
    "adv_model = ProjectionDiscriminator(adv_model,144,2,)\n",
    "\n",
    "adv_model.cuda()\n",
    "        \n",
    "optim_dist = th.optim.Adam(\n",
    "                          [\n",
    "    {'params': means_per_cluster + stds_per_cluster,\n",
    "    'lr': 1e-2,\n",
    "    'weight_decay': 0},], betas=(0,0.9))\n",
    "\n",
    "\n",
    "optim_adv = th.optim.Adam([{\n",
    "    'params': adv_model.parameters(),\n",
    "    'lr': 4e-3, 'weight_decay': 0.00}],#lr 0.0004\n",
    "                         betas=(0,0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reversible.gaussian import get_gauss_samples\n",
    "from reversible.uniform import get_uniform_samples\n",
    "from reversible.revnet import invert \n",
    "import pandas as pd\n",
    "from gradient_penalty import gradient_penalty\n",
    "import time\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "g_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "g_grad = np.nan\n",
    "d_loss = np_to_var([np.nan],dtype=np.float32)\n",
    "d_grad = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "other_outs[:,500] = means_per_cluster[i_class][500].unsqueeze(0) + (\n",
    "    th.randn(len(other_outs)).cuda() * stds_per_cluster[i_class][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10001\n",
    "for i_epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    optim_adv.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    optim_dist.zero_grad()\n",
    "    for i_class in range(len(inputs)):\n",
    "        mean = means_per_cluster[i_class]\n",
    "        log_std = stds_per_cluster[i_class]\n",
    "        if i_class == 1:\n",
    "            mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                         means_per_cluster[0][501:]))\n",
    "            log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                         stds_per_cluster[0][501:]))\n",
    "        std = th.exp(log_std)\n",
    "        this_inputs = inputs[i_class]\n",
    "        y = np_to_var([i_class]).cuda()\n",
    "        samples = get_gauss_samples(len(this_inputs), mean, std, truncate_to=3)\n",
    "        other_outs = feature_model(inputs[i_class - 1])\n",
    "        \n",
    "        # move to this class\n",
    "        other_outs[:,500] = means_per_cluster[i_class][500].unsqueeze(0) + (\n",
    "            th.randn(len(other_outs)).cuda() * stds_per_cluster[i_class][500])\n",
    "        samples = th.cat((samples, other_outs), dim=0)\n",
    "\n",
    "        inverted = invert(feature_model, samples)\n",
    "        score_fake = adv_model(inverted, y)\n",
    "        if (i_epoch % 10) != 0:\n",
    "            score_real = adv_model(this_inputs, y)\n",
    "            gradient_loss = gradient_penalty(adv_model, this_inputs, inverted[:(len(this_inputs))], y)\n",
    "            d_loss = -score_real.mean() + score_fake.mean() + gradient_loss * 10\n",
    "            d_loss.backward()\n",
    "            d_grad = np.mean([th.sum(p.grad **2).item() for p in adv_model.parameters()])\n",
    "        else:\n",
    "            g_loss = -th.mean(score_fake)\n",
    "            g_loss.backward()\n",
    "            g_grad = np.mean([th.sum(p.grad **2).item() for p in feature_model.parameters()])\n",
    "    if (i_epoch % 10) != 0:\n",
    "            optim_adv.step()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "        optim_dist.step()\n",
    "    with th.no_grad():\n",
    "        sample_wd_row = {}\n",
    "        for i_class in range(len(inputs)):\n",
    "            this_inputs = inputs[i_class]\n",
    "            mean = means_per_cluster[i_class]\n",
    "            log_std = stds_per_cluster[i_class]\n",
    "            if i_class == 1:\n",
    "                mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                             means_per_cluster[0][501:]))\n",
    "                log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                             stds_per_cluster[0][501:]))\n",
    "            std = th.exp(log_std)\n",
    "            y = np_to_var([i_class]).cuda()\n",
    "            samples = get_gauss_samples(len(this_inputs), mean, std, truncate_to=3)\n",
    "\n",
    "            inverted = invert(feature_model, samples)\n",
    "            in_np = var_to_np(this_inputs).reshape(len(this_inputs), -1)\n",
    "            fake_np = var_to_np(inverted).reshape(len(inverted), -1)\n",
    "            import ot\n",
    "\n",
    "            dist = np.sqrt(np.sum(np.square(in_np[:,None] - fake_np[None]), axis=2))\n",
    "            match_matrix = ot.emd([],[], dist)\n",
    "            cost = np.sum(dist * match_matrix)\n",
    "            score_fake = adv_model(inverted, y)\n",
    "            score_real = adv_model(inputs[i_class], y)\n",
    "            wd_dist = th.mean(score_real) - th.mean(score_fake)\n",
    "            sample_wd_row.update({\n",
    "                'wd_dist_' + str(i_class): wd_dist.item() ,\n",
    "                'sampled_wd' + str(i_class): cost,\n",
    "                'wd_diff' + str(i_class): cost - wd_dist.item(),\n",
    "            })\n",
    "        end_time = time.time()\n",
    "        epoch_row = {\n",
    "        'd_loss': d_loss.item(),\n",
    "        'g_loss': g_loss.item(),\n",
    "        'o_real': th.mean(score_real).item(),\n",
    "        'o_fake': th.mean(score_fake).item(),\n",
    "        'g_grad': g_grad,\n",
    "        'd_grad': d_grad,\n",
    "        'runtime': end_time -start_time,}\n",
    "        epoch_row.update(sample_wd_row)\n",
    "        df = df.append(epoch_row, ignore_index=True)\n",
    "        if i_epoch % (max(1,n_epochs // 100)) == 0:\n",
    "            display_text(\"Epoch {:d}\".format(i_epoch))\n",
    "            display(df.iloc[-5:])\n",
    "        if i_epoch % (n_epochs // 20) == 0:\n",
    "            print(\"stds\\n\", var_to_np(th.exp(th.stack(stds_per_cluster))))\n",
    "            for i_class in range(len(stds_per_cluster)):\n",
    "                mean = means_per_cluster[i_class]\n",
    "                log_std = stds_per_cluster[i_class]\n",
    "                if i_class == 1:\n",
    "                    mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                                 means_per_cluster[0][501:]))\n",
    "                    log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                                 stds_per_cluster[0][501:]))\n",
    "                std = th.exp(log_std)\n",
    "                y = np_to_var([i_class]).cuda()\n",
    "                samples = get_gauss_samples(len(this_inputs), mean, std, truncate_to=3)\n",
    "                inverted = invert(feature_model, samples).squeeze()\n",
    "                image_grid = np.zeros((5,10,28,28))\n",
    "                for i_row in range(image_grid.shape[0]):\n",
    "                    for i_col in range(image_grid.shape[1]):\n",
    "                        image_grid[i_row, i_col] = var_to_np(inverted[i_row * image_grid.shape[1] + i_col])\n",
    "                im = create_bw_image(image_grid).resize((image_grid.shape[1]*50,image_grid.shape[0]*50))\n",
    "                display(im)\n",
    "\n",
    "            for i_class in range(len(inputs)):\n",
    "                this_inputs = inputs[i_class]\n",
    "                mean = means_per_cluster[i_class]\n",
    "                log_std = stds_per_cluster[i_class]\n",
    "                if i_class == 1:\n",
    "                    mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                                 means_per_cluster[0][501:]))\n",
    "                    log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                                 stds_per_cluster[0][501:]))\n",
    "                std = th.exp(log_std)\n",
    "                y = np_to_var([i_class]).cuda()\n",
    "                samples = get_gauss_samples(len(this_inputs) * 5, mean, std, truncate_to=3)\n",
    "\n",
    "                inverted = invert(feature_model, samples)\n",
    "                in_np = var_to_np(this_inputs).reshape(len(this_inputs), -1)\n",
    "                fake_np = var_to_np(inverted).reshape(len(inverted), -1)\n",
    "                import ot\n",
    "\n",
    "                dist = np.sqrt(np.sum(np.square(in_np[:,None] - fake_np[None]), axis=2))\n",
    "                match_matrix = ot.emd([],[], dist)\n",
    "                cost = np.sum(dist * match_matrix)\n",
    "                print(\"sliced\", cost)\n",
    "            a = means_per_cluster[0].detach()\n",
    "            a = a.repeat(50,1)\n",
    "            a.data[:,500] = th.linspace(means_per_cluster[0].data[500], means_per_cluster[1].data[500], a.shape[0])\n",
    "            inverted = var_to_np(invert(feature_model, a).squeeze())\n",
    "            display(create_bw_image(inverted.reshape(5,10,28,28)).resize((500,250)))\n",
    "            for i_class in range(2):\n",
    "                this_outs = feature_model(inputs[i_class],)\n",
    "\n",
    "                this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "\n",
    "                inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "\n",
    "                image_grid = np.stack((var_to_np(inputs[i_class]).squeeze(), inverted), axis=0)\n",
    "                display(create_bw_image(image_grid.swapaxes(0,1).reshape(10,10,28,28)).resize((500,500)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "\n",
    "    image_grid = np.stack((var_to_np(inputs[i_class]).squeeze(), inverted), axis=0)\n",
    "    display(create_bw_image(image_grid.swapaxes(0,1).reshape(10,10,28,28)).resize((500,500)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "    angles_inv = np.array([angle_of_image(a) for a in inverted])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in var_to_np(inputs[i_class]).squeeze()])\n",
    "angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(np.sin(angle_diffs), np.cos(angle_diffs), ls='', marker='o')\n",
    "plt.ylim(-1.25,1.25)\n",
    "plt.xlim(-0.25,1.25)\n",
    "print(np.mean(angle_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "    this_inputs = var_to_np(inputs[i_class].squeeze())\n",
    "    inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "    angles_inv = np.array([angle_of_image(a) for a in inverted])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in this_inputs])\n",
    "    for i_in in range(len(inverted)):\n",
    "        im_pair = np.stack((this_inputs[i_in], inverted[i_in]), axis=0)\n",
    "        display(create_bw_image(im_pair[None,:]).resize((88,44)))\n",
    "        print(angles_ins[i_in], angles_inv[i_in])\n",
    "    angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.plot(np.sin(angle_diffs), np.cos(angle_diffs), ls='', marker='o')\n",
    "    plt.ylim(-1.25,1.25)\n",
    "    plt.xlim(-0.25,1.25)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "    print(np.mean(angle_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_inputs = var_to_np(inputs[i_class].squeeze())\n",
    "    mean = means_per_cluster[1-i_class]\n",
    "    log_std = stds_per_cluster[1-i_class]\n",
    "    if i_class == 1:\n",
    "        mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                     means_per_cluster[0][501:]))\n",
    "        log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                     stds_per_cluster[0][501:]))\n",
    "    std = th.exp(log_std)\n",
    "    y = np_to_var([i_class]).cuda()\n",
    "    samples = get_gauss_samples(len(inputs[1-i_class]), mean, std, truncate_to=3)\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, samples)).squeeze()\n",
    "    angles_inv = np.array([angle_of_image(a) for a in inverted])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in this_inputs])\n",
    "    for i_in in range(len(inverted)):\n",
    "        im_pair = np.stack((this_inputs[i_in], inverted[i_in]), axis=0)\n",
    "        display(create_bw_image(im_pair[None,:] > 0.7).resize((88,44)))\n",
    "        print(angles_ins[i_in], angles_inv[i_in])\n",
    "    angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.plot(np.sin(angle_diffs), np.cos(angle_diffs), ls='', marker='o')\n",
    "    plt.ylim(-1.25,1.25)\n",
    "    plt.xlim(-0.25,1.25)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "    print(np.mean(angle_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    mean = means_per_cluster[1-i_class]\n",
    "    log_std = stds_per_cluster[1-i_class]\n",
    "    if i_class == 1:\n",
    "        mean = th.cat((means_per_cluster[0][0:500], mean[500:501],\n",
    "                     means_per_cluster[0][501:]))\n",
    "        log_std = th.cat((stds_per_cluster[0][0:500], log_std[500:501],\n",
    "                     stds_per_cluster[0][501:]))\n",
    "    std = th.exp(log_std)\n",
    "    y = np_to_var([i_class]).cuda()\n",
    "    samples = get_gauss_samples(len(inputs[1-i_class]), mean, std, truncate_to=3)\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, samples)).squeeze()\n",
    "    angles_inv = np.array([angle_of_image(a) for a in inverted])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in var_to_np(inputs[i_class]).squeeze()])\n",
    "    angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "    print(np.mean(angle_diffs))\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.plot(np.sin(angle_diffs), np.cos(angle_diffs), ls='', marker='o')\n",
    "    plt.ylim(-1.25,1.25)\n",
    "    plt.xlim(-0.25,1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def angle_of_image(image):\n",
    "    y,x = np.nonzero(image > 0.7)\n",
    "    # now x = width, y = height\n",
    "    y = 28 - y # to count height from bottom\n",
    "    xy = np.stack((x,y)).T\n",
    "    xy = xy -14\n",
    "    pca = sklearn.decomposition.PCA().fit(xy)\n",
    "    return (np.angle(pca.components_[0][0]+ pca.components_[0][1]*1j, deg=False)) % np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [np_to_var(X_train_topo[y_train == 0][50:], dtype=np.float32).cuda(),\n",
    "      np_to_var(X_train_topo[y_train == 1][50:], dtype=np.float32).cuda()]\n",
    "both_wanted_ims = []\n",
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "\n",
    "    plt.plot(var_to_np(this_outs)[:,500], var_to_np(this_outs)[:,500] * 0, ls='', marker='o',\n",
    "            color=seaborn.color_palette()[i_class])\n",
    "    test_outs = feature_model(test_inputs[i_class],)\n",
    "    plt.plot(var_to_np(test_outs)[:,500], var_to_np(test_outs)[:,500] * 0, ls='', marker='x',\n",
    "            color=seaborn.color_palette()[i_class])\n",
    "    \n",
    "    sorted_vals, i_sorted = th.sort(test_outs[:,500])\n",
    "    wanted_inds = np.int64(np.round(np.linspace(0, len(i_sorted)-1,20)))\n",
    "    wanted_inds = i_sorted[wanted_inds]\n",
    "\n",
    "    wanted_ims = var_to_np(test_inputs[i_class][wanted_inds].squeeze())\n",
    "    display(create_bw_image(wanted_ims[None]))\n",
    "    both_wanted_ims.append(wanted_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bw_image(np.concatenate(np.array(both_wanted_ims)[::-1])[None]).resize((40*2*28,28*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "    other_outs = feature_model(inputs[1-i_class])\n",
    "    diffs = this_outs.unsqueeze(1) - other_outs.unsqueeze(0)\n",
    "    diffs.data[:,:,500]= 0\n",
    "    diffs = th.sum(diffs * diffs, dim=2)\n",
    "    _, i_pairs = th.min(diffs,dim=1)\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "    image_grid = np.stack((var_to_np(inputs[i_class]).squeeze(), var_to_np(inputs[1-i_class][i_pairs]).squeeze()), axis=0)\n",
    "    display(create_bw_image(image_grid.swapaxes(0,1).reshape(10,10,28,28)).resize((500,500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "\n",
    "    inverted = var_to_np(invert(feature_model, this_outs).squeeze())\n",
    "\n",
    "    image_grid = np.stack((var_to_np(inputs[i_class]).squeeze(), inverted), axis=0)\n",
    "    display(create_bw_image(image_grid.swapaxes(0,1).reshape(10,10,28,28)).resize((500,500)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    this_outs = feature_model(inputs[i_class],)\n",
    "    this_outs.data[:,500] = means_per_cluster[1-i_class].data[500]\n",
    "    other_outs = feature_model(inputs[1-i_class])\n",
    "    diffs = this_outs.unsqueeze(1) - other_outs.unsqueeze(0)\n",
    "    diffs.data[:,:,500]= 0\n",
    "    diffs = th.sum(diffs * diffs, dim=2)\n",
    "    _, i_pairs = th.min(diffs,dim=1)\n",
    "\n",
    "    image_grid = np.stack((var_to_np(inputs[i_class]).squeeze(), var_to_np(inputs[1-i_class][i_pairs]).squeeze()), axis=0)\n",
    "    display(create_bw_image(image_grid.swapaxes(0,1).reshape(10,10,28,28)).resize((500,500)))\n",
    "    \n",
    "    \n",
    "    angles_inv = np.array([angle_of_image(a) for a in var_to_np(inputs[i_class]).squeeze()])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in var_to_np(inputs[1-i_class][i_pairs]).squeeze()])\n",
    "    angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "    print(np.mean(angle_diffs))\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.plot(np.sin(angle_diffs), np.cos(angle_diffs), ls='', marker='o')\n",
    "    plt.ylim(-1.25,1.25)\n",
    "    plt.xlim(-0.25,1.25)\n",
    "    \n",
    "    angles_inv = np.array([angle_of_image(a) for a in var_to_np(inputs[i_class]).squeeze()])\n",
    "    angles_ins = np.array([angle_of_image(a) for a in var_to_np(inputs[1-i_class][i_pairs[th.randperm(len(i_pairs))]]).squeeze()])\n",
    "    angle_diffs = np.abs(angles_ins - angles_inv)\n",
    "    print(np.mean(angle_diffs))\n",
    "    print(np.corrcoef(np.mean(var_to_np(inputs[i_class].squeeze()), axis=(1,2)),\n",
    "            np.mean(var_to_np(inputs[1-i_class][i_pairs]).squeeze(), axis=(1,2)),)[0,1])\n",
    "    print(np.corrcoef(np.mean(var_to_np(inputs[i_class].squeeze()), axis=(1,2)),\n",
    "            np.mean(var_to_np(inputs[1-i_class][i_pairs[th.randperm(len(i_pairs))]]).squeeze(), axis=(1,2)),)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take a 0, move it to 1 see what it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also check accuracy of thresholded classifier on that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(th.exp(stds_per_cluster[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.exp(stds_per_cluster[1])[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_per_cluster[1][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_per_cluster[0][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = means_per_cluster[0].detach()\n",
    "\n",
    "a = a.repeat(50,1)\n",
    "\n",
    "a.data[:,500] = th.linspace(means_per_cluster[0].data[500], means_per_cluster[1].data[500], a.shape[0])\n",
    "\n",
    "inverted = var_to_np(invert(feature_model, a).squeeze())\n",
    "\n",
    "create_bw_image(inverted.reshape(5,10,28,28)).resize((1000,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bw_image(inverted.reshape(5,10,28,28)).resize((1000,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_to_np(th.exp(stds_per_cluster[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
